{"cells":[{"cell_type":"markdown","id":"Thcx6MGstdDx","metadata":{"id":"Thcx6MGstdDx"},"source":["#FICO Analytic Challenge Week 4.5 Solutions\n","##Â© Fair Isaac 2024"]},{"cell_type":"code","execution_count":null,"id":"lUS8IqBxNnAz","metadata":{"id":"lUS8IqBxNnAz"},"outputs":[],"source":["import os\n","import sys\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":null,"id":"XFvc7Boczxkz","metadata":{"id":"XFvc7Boczxkz"},"outputs":[],"source":["# Defining projects path and directory locations\n","path = '/content/drive/MyDrive/FICO Analytic Challenge/'\n","sys.path.append(path +'Data')\n","sys.path.append(path +'Week 04')\n","os.chdir(path)\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"id":"18523458","metadata":{"id":"18523458"},"outputs":[],"source":["# Data and numerical libararies\n","import pandas as pd\n","import numpy as np\n","\n","# Plotting and stats library\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import ttest_ind\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Removing limitation in viewing pandas columns and rows\n","pd.set_option('display.max_columns', None, 'display.max_rows', None)"]},{"cell_type":"code","execution_count":null,"id":"UEjjb-jhzfTC","metadata":{"id":"UEjjb-jhzfTC"},"outputs":[],"source":["# Folder's name that's holding dataset\n","data = 'Data'"]},{"cell_type":"code","source":["# dataset file prefix\n","trainFile = ['train']\n","testFile = ['test_A']\n","\n","# CSV filename and where features dataset will be saved\n","featureTrainFileSuffix=\"_advanced_features.csv\"\n","featureTestFileSuffix=\"_advanced_features.csv\"\n","\n","filePathTrain=os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n","filePathTest=os.path.join(path + data, testFile[0] + featureTestFileSuffix)\n","\n","if not os.path.isfile(filePathTrain):\n","    featureTrainFileSuffix=\"_features.csv\"\n","\n","if not os.path.isfile(filePathTest):\n","    featureTestFileSuffix=\"_features.csv\"\n","\n","trainsaveCSV = os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n","testsaveCSV = os.path.join(path + data, testFile[0] + featureTestFileSuffix)"],"metadata":{"id":"elIT1TynZ04h"},"id":"elIT1TynZ04h","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to import dataset\n","def modify_df(path, data, filename, featureFileSuffix):\n","    filePath=os.path.join(path + data, filename[0] + featureFileSuffix)\n","\n","    df1 = pd.read_csv(filePath)\n","    df1['transactionDateTime'] = pd.to_datetime(df1['transactionDateTime']).astype('datetime64[ns]')\n","    df1 = df1.sort_values(by=['pan','transactionDateTime'])\n","\n","    return df1"],"metadata":{"id":"pPti0rEAZ9jy"},"id":"pPti0rEAZ9jy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c9eee856","metadata":{"id":"c9eee856","scrolled":true},"outputs":[],"source":["#Upload pre-processed data\n","df1 = modify_df(path, data, trainFile, featureTrainFileSuffix)\n","df2 = modify_df(path, data, testFile, featureTestFileSuffix)"]},{"cell_type":"code","execution_count":null,"id":"05f7601a","metadata":{"id":"05f7601a"},"outputs":[],"source":["#Combine dataframes to be processed together\n","df = pd.concat([df1,df2])"]},{"cell_type":"code","execution_count":null,"id":"9f998fd4","metadata":{"id":"9f998fd4"},"outputs":[],"source":["#It often helps to decompose the datetime into more useful fields\n","df['datetime'] =  pd.to_datetime(df['transactionDateTime'])\n","df['transactionHour'] = df['datetime'].dt.hour\n","\n","#Since many of our features will be calculated at the account level, let's sort our features accordingly\n","df = df.sort_values(by=['pan', 'transactionDateTime'])\n","\n","#When reordering data, it's customary to reset the index to align with the order of the rows\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"1tAhQy1nP4LB","metadata":{"id":"1tAhQy1nP4LB"},"outputs":[],"source":["#Sometimes it's convenient to create an array of the base variables before we start defining features\n","base_cols = ['pan', 'merchant', 'category', 'transactionAmount', 'first', 'last', 'mdlIsFraudTrx', 'mdlIsFraudAcct',\n","             'is_train', 'cardholderCountry', 'cardholderState', 'transactionDateTime', 'gender',\n","             'street', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n","             'merch_lat', 'merch_long', 'merchCountry', 'merchState', 'deltaTime']"]},{"cell_type":"code","execution_count":null,"id":"yd3EAjqqRuFY","metadata":{"id":"yd3EAjqqRuFY"},"outputs":[],"source":["df['amount_diff'] = df.groupby('pan')['transactionAmount'].diff()\n","\n","df['datetime'] =  pd.to_datetime(df['transactionDateTime'])\n","df.set_index('datetime', inplace=True)\n","\n","df['num_last_24_hours'] = df.groupby('pan')['transactionAmount'].apply(lambda x: x.rolling(window='24h').count()).reset_index(level=0, drop=True)\n","\n","df.reset_index(inplace=True)\n","df.set_index('datetime', inplace=True)\n","df = df[~df.index.duplicated(keep='first')]"]},{"cell_type":"code","execution_count":null,"id":"ICWP-lPuGQAH","metadata":{"id":"ICWP-lPuGQAH"},"outputs":[],"source":["# Here, we're grouping by pan, then taking a 24 hour rolling window for each record using the \"transactionAmount\" column, then counting for \"transactionAmount\" > 100\n","# Sort by timestamp to enable accurate rolling calculations within each group\n","df.reset_index(inplace=True)\n","df.set_index('datetime', inplace=True)\n","df = df.sort_values(by=['pan', 'transactionDateTime'])\n","\n","# Define the rolling window criteria function\n","def rolling_count_fill(x, hour='1h'):\n","    # Apply rolling window and count values > 100\n","    rolling_counts = x.rolling(hour, on='transactionDateTime')['transactionAmount'] \\\n","        .apply(lambda y: (y > 100).sum(), raw=True)\n","\n","    # Replace NaN: carry forward last valid count or use 0 if none\n","    filled_counts = rolling_counts.fillna(0).where(rolling_counts > 0, rolling_counts.ffill().fillna(0))\n","    return filled_counts\n","\n","# Group by 'pan' and apply the rolling count with NaN handling\n","df['num_hi_amt_last_hour'] = df.groupby('pan', group_keys=False).apply(rolling_count_fill, '1h')"]},{"cell_type":"code","execution_count":null,"id":"ed2XMJmsGPzi","metadata":{"id":"ed2XMJmsGPzi"},"outputs":[],"source":["def calculate_category_ratio(df):\n","\n","    # Define a function to compute rolling category ratios for each customer\n","    def compute_ratios(group):\n","        # Create a boolean mask of matches within the past 5 records (rolling window)\n","        match_mask = (group['category'] == group['category'].shift(1))\n","        # Rolling sum of matches for past 5 rows\n","        rolling_sum = match_mask.rolling(window=5, min_periods=1).sum()\n","        # Calculate the ratio\n","        return rolling_sum / rolling_sum.rolling(window=5, min_periods=1).count()\n","\n","    # Apply the function group-wise (for each customer)\n","    df['category_ratio'] = df.groupby('pan', group_keys=False).apply(compute_ratios)\n","\n","    return df\n","df = calculate_category_ratio(df)\n","df.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"UnQiAiOe3POB","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1730761376019,"user":{"displayName":"Quintin","userId":"08638758343693318412"},"user_tz":480},"id":"UnQiAiOe3POB","outputId":"2c0c0b06-88c5-4249-b811-714181b3e4d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Features to save: ['pan', 'merchant', 'category', 'transactionAmount', 'first', 'last', 'mdlIsFraudTrx', 'mdlIsFraudAcct', 'is_train', 'cardholderCountry', 'cardholderState', 'transactionDateTime', 'gender', 'street', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'merchCountry', 'merchState', 'deltaTime', 'amt_trend_5e', 'amount_diff', 'repeat_amt', 'is_international', 'category_ratio', 'amt_trend_24h', 'IsHighValue', 'IS_0_TO_5AM', 'RelativeAmount', 'num_hi_amt_last_hour', 'num_last_24_hours', 'is_gas', 'is_cnp', 'is_late_night', 'count_trend_1h', 'transactionHour', 'user_avg_amount']\n"]}],"source":["# Features to save\n","if \"datetime\" in df.columns:\n","  features = list(set(df.columns) - set(base_cols + [\"datetime\"]))\n","else:\n","  features = list(set(df.columns) - set(base_cols))\n","\n","saveFeatures = [*base_cols, *features]\n","print(f\"Features to save: {saveFeatures}\")"]},{"cell_type":"code","execution_count":null,"id":"ac17ea4b","metadata":{"id":"ac17ea4b"},"outputs":[],"source":["df[df['is_train'] == 1][saveFeatures].to_csv(trainsaveCSV ,index=False)\n","df[df['is_train'] == 0][saveFeatures].to_csv(testsaveCSV,index=False)"]},{"cell_type":"markdown","id":"93AVTvhTNxGz","metadata":{"id":"93AVTvhTNxGz"},"source":["# <font color='red'>TIP</font>\n","### Make a code block below that will read in a dataset and perform all calculations needed, using the code that's working to generate the desired feature(s). This will make feature engineering faster for new datasets or old ones that you'd like to add more features to. Then have it save the dateset with features to desired location."]},{"cell_type":"code","source":[],"metadata":{"id":"9uVOWCQUWH99"},"id":"9uVOWCQUWH99","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1j4gGzYrypyy-xUzeNXZlIF_-OSxZCt94","timestamp":1730853939090}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}