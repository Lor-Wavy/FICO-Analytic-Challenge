{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b3c7e9",
   "metadata": {
    "id": "07b3c7e9"
   },
   "source": [
    "# **FICO Analytic Challenge © Fair Isaac 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6255ef9",
   "metadata": {
    "id": "f6255ef9"
   },
   "source": [
    "# Week 10: Performance Metrics on Blind Holdout Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb4e14",
   "metadata": {
    "id": "1ceb4e14"
   },
   "source": [
    "## Model Performance Metrics Importance\n",
    "\n",
    "In the past weeks we introduced Logistic Regression, Neural Networks, inference and explainability. This week we'll focus on model performance.\n",
    "\n",
    "In the credit fraud detection world, our primary goal is to accurately identify fraudulent activities while minimizing the impact on genuine transactions. To achieve this, we rely on machine learning models to sift through the vast amounts of transaction data and flag suspicious activities. But how do we know if our models are effective? This is where model performance metrics come into play, serving as vital tools to evaluate and compare the effectiveness of our models. This week, we'll dive into why these metrics are crucial and why, in the fraud detection industry, we often need to develop specific metrics to ensure we deliver the best models that provide the highest value to our clients.\n",
    "\n",
    "### Why Model Performance Metrics Matter\n",
    "\n",
    "- **True Positive Rate (TPR)** measures how well the model is correctly identifying actual cases of fraud.\n",
    "- **False Positive Rate (FPR)** shows how often the model incorrectly labels normal transactions as fraud.\n",
    "\n",
    "These metrics help us understand the trade-offs between catching fraudsters and avoiding unnecessary disruptions for genuine customers.\n",
    "\n",
    "### Custom Metrics: Necessity for Accurate Evaluation\n",
    "\n",
    "Credit fraud detection models are often evaluated using neural networks, where the predictions are presented as scores typically ranging from 1 to 1000. These scores help us rank transactions by their likelihood of being fraud. However, comparing scores from different models can be tricky. Each model might assign scores differently, making it hard to directly compare one model’s scoring to another’s.\n",
    "\n",
    "To address this, we need to create metrics that can compare models more holistically. One approach is to use cumulative gain or lift charts, which show the what percentage of fraud is caught as we move through the score range. By comparing the areas under these curves, we can get a better sense of overall performance.\n",
    "\n",
    "Additionally, creating metrics that account for the specific cost-benefit scenarios of our use case can ensure that we are making informed decisions. By developing custom metrics and carefully evaluating score distributions, we can make sure our models are not just effective but also finely tuned for catching real-world credit fraud.\n",
    "\n",
    "In the fraud detection field, there are industry standard performance metrics that give meaningful insight. We'll focus on a few key performance metrics that are especially useful for understanding these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48384f77",
   "metadata": {
    "id": "48384f77"
   },
   "source": [
    "## False-Positive and Detection Rates\n",
    "- Assessing the performance of a model is a matter of performing a cost-benefit analysis.\n",
    "- The cost involves the number of false positives (FP), which are normal transactions mistakenly tagged as fraud. The benefit is correct fraud predictions and the reduction in fraud losses achieved by acting upon those predictions.\n",
    "- Ideally, we want our model to increase the number of correct fraud predictions without raising, or even reducing, the number of false positives.\n",
    "- If a model scores at least one transaction on a fraud account above a suspect threshold score, that fraudulent account is considered to be detected.\n",
    "\n",
    "### Transaction Based Metrics\n",
    "- **Percent Non-Fraud (%NF):** False positives are measured using a Percent Non-Fraud metric.\n",
    "    - This percentage is the number of transactions from non-fraud accounts that scored above our suspect threashold, divided by the total number of transactions from these non-fraud accounts.\n",
    "    - As the threshold score increases, the number of false positives decreases, but also reduces the number of actual frauds detected.\n",
    "      - For instance, if a bank raises the threshold to reduce false alarms, they might also miss catching some genuine fraud cases.\n",
    "- **Transaction Value Detection Rate (TVDR):** This percentage shows us how much money involved in fraud transactions is caught by our model. It looks at the transactions that score above a certain threshold and calculates what percentage of the total fraud amount these represent.\n",
    "  - For example, if 100k of fraudulent transactions occur and the model identifies 80k of it, the TVDR would be 80%.\n",
    "- **Transaction Detection Rate (TDR) or Percent Fraud (%F):** The percentage of fraud transactions with scores above a score threshold.\n",
    "    - If there are 100 fraud transactions, and the model correctly identifies 72 of them, the TDR is 72 percent.\n",
    "\n",
    "### Account Based Metrics\n",
    "- **Account Percent Non-Fraud (A%NF):** Similar to %NF but at the account level, this measures the number of non-fraud accounts that score above the threshold, divided by the total number of non-fraud accounts. This helps in understanding how often legitimate accounts are mistakenly flagged as suspicious.\n",
    "- **Account Detection Rate (ADR):**  The percentage of correctly identified fraud accounts. This is calculated by taking the Number of frauds accounts correctly detected at or above some Score Threshold divided by total number of actual fraud accounts.\n",
    "    - For example, if there are 100 fraud accounts and the model identifies 80, the ADR is 80%.\n",
    "- **Value Detection Rate (VDR):** Sum value associated with frauds detected at some Score Threshold divided by the sum value associated with all frauds. Essentially, this tells us what fraction of the total fraudulent dollars was successfully identified by the model.\n",
    "  - For example, if the total amount of fraud is 100k and the model catches frauds totaling 80k, the VDR would be 80%.\n",
    "    - There are two types: OLVDR (on-line) and RTVDR (real-time)\n",
    "- **Real-Time Value Detection Rate (RTVDR):** This measures the amount of money saved from correct fraud predictions, expressed as a percentage of the total amount fraudulently charged against accounts. Only amounts associated with approved fraud transactions are counted.\n",
    "    - Lowering the threshold score generally improves RTVDR but may increase false positives, as more transactions are flagged as potential fraud.\n",
    "\n",
    "## Types of Measurements\n",
    "\n",
    "### Receiver Operating Characteristic (ROC) Curve\n",
    "- Created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "- Curves closer to the top-left corner indicate a better performance.\n",
    "    - The diagonal line (FPR = TPR) represents a random classifier; the closer the curve is to this line, the less accurate the model.\n",
    "\n",
    "### Area Under Curve (AUC)  \n",
    "- Specifically, we’re looking at the area under the Receiver Operating Characteristic (ROC) curve.\n",
    "- This area, ranging from 0 to 1, measures the ability of a classification model to separate the two classes and sift signal from noise.\n",
    "- To first draw the ROC curve, we plot the True Positive Rate (TPR) against the False Positive Rate (FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NlwPPpjutipt",
   "metadata": {
    "id": "NlwPPpjutipt"
   },
   "source": [
    "## Mount the Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZbpimXWdzI4",
   "metadata": {
    "id": "tZbpimXWdzI4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "path = '/content/drive/MyDrive/FICO Analytic Challenge/'\n",
    "sys.path.append(path +'Data')\n",
    "sys.path.append(path +'Week 04')\n",
    "sys.path.append(path +'Week 06')\n",
    "sys.path.append(path +'Week 07')\n",
    "sys.path.append(path +'Week 10')\n",
    "os.chdir(path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcc763",
   "metadata": {
    "id": "a5bcc763"
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad145907",
   "metadata": {
    "id": "ad145907"
   },
   "outputs": [],
   "source": [
    "# import the necessary libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import dump, load\n",
    "from fico_functions import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Removing limitation in viewing pandas columns and rows\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91807ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to model\n",
    "mdlPath = f\"{path}Model\"\n",
    "\n",
    "# Folder's name that's holding data of interest\n",
    "data = 'Data'\n",
    "\n",
    "# Model name; this will be used to distinguish model's output files\n",
    "model='NNet'\n",
    "\n",
    "# import scale file \n",
    "scaleFile = os.path.join(path + data, 'scaler.' + model + '.' + data + \".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bcc75",
   "metadata": {
    "id": "775bcc75"
   },
   "source": [
    "### Data location\n",
    "- test_C_notags.csv is the blind holdout dataset\n",
    "    - you should have already created the features for it and named it either of the following:\n",
    "        - test_C_notags_features.csv\n",
    "            - if only using features from week 4\n",
    "        - test_C_notags_advanced_features.csv\n",
    "            - if also using week 8\n",
    "- score.NNet.test_C_notags_features.csv or score.NNet.test_C_notags_advanced_features.csv\n",
    "    - this should have scores from your trained NNet model\n",
    "    - this dataset doesn't have the following columns since it has \"notags\"\n",
    "        - mdlIsFraudTrx\n",
    "        - mdlIsFraudAcct\n",
    "- score.NNet.test_C_features.csv or score.NNet.test_C_advanced_features.csv\n",
    "    - this is the files name that we return to you which includes the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b49662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to correct file name\n",
    "blindholdoutFile = ['test_C']\n",
    "\n",
    "# CSV filename suffex\n",
    "featureTestFileSuffix=\"_advanced_features.csv\"\n",
    "\n",
    "# Scored Blind Holdout file location\n",
    "blindholdoutCSV = os.path.join(path + data, 'score.' + model + '.' + blindholdoutFile[0] + featureTestFileSuffix)\n",
    "\n",
    "if not os.path.isfile(blindholdoutCSV):\n",
    "    featureTestFileSuffix=\"_features.csv\"\n",
    "    blindholdoutCSV = os.path.join(path + data, 'score.' + model + '.' + blindholdoutFile[0] + featureTestFileSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f717855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "df_test = import_df(blindholdoutCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089cf13",
   "metadata": {
    "id": "d089cf13"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e66554",
   "metadata": {
    "id": "34e66554"
   },
   "source": [
    "### Removing Non-Fraud Transactions from Fraud Accounts\n",
    "\n",
    "A fraud account will have transactions that are fraud and non-fraud. To ensure we dont have any uncertainty with a transaction being non-fraud or not in a fraud account, we remove the records that have non-fraud transaction from fraud accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0973f7",
   "metadata": {
    "id": "9a0973f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = filterNFTrxfromFAccn(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d30f3",
   "metadata": {
    "id": "291d30f3"
   },
   "outputs": [],
   "source": [
    "df_test = matureProf_n_months(df_test, 'transactionDateTime', ['pan','transactionDateTime'], n_months=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867fe21",
   "metadata": {
    "id": "8867fe21"
   },
   "outputs": [],
   "source": [
    "print(\"\\033[1mNNet\\033[0m\")\n",
    "dataset_count(df_test, df_isTrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95119a9",
   "metadata": {
    "id": "a95119a9"
   },
   "source": [
    "### Threshold Score Value\n",
    "We're working with metrics that vary based on a threshold score. Our goal is to calculate these values for a specific range of threshold scores, which will then be used to generate the ROC plot. This can help reduce False Positives (FP) by adjusting final outputs to the desired Threshold Score where any transaction scoring at or above the threshold would be considered a fraud transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1aae5",
   "metadata": {
    "id": "96c1aae5"
   },
   "outputs": [],
   "source": [
    "# Getting a range of threshold scores\n",
    "threshold_list = list(range(0,980, 5))\n",
    "threshold_list.extend(range(980, 1000, 1))\n",
    "\n",
    "# Removing any duplicated and sorting list\n",
    "threshold_list = sorted(set(threshold_list))\n",
    "# print(threshold_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51849692",
   "metadata": {
    "id": "51849692"
   },
   "source": [
    "## Calculate Test Sets Metrics\n",
    "\n",
    "<font color='red'>(**Warning: Takes a long time**)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eda8df",
   "metadata": {
    "id": "79eda8df"
   },
   "outputs": [],
   "source": [
    "# Dataset to calculate performance metrics for train and required columns\n",
    "trainOrTest= 'test'\n",
    "perfCols = ['pan', 'is_train', 'mdlIsFraudTrx', 'mdlIsFraudAcct', 'transactionAmount', 'transactionDateTime', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012105b5",
   "metadata": {
    "id": "012105b5"
   },
   "outputs": [],
   "source": [
    "pNF, TDR, TVDR, ApNF, ADR, RTVDR = calcMetrics(df_test[perfCols], threshold_list, model, df_isTrain=trainOrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e1ff6",
   "metadata": {
    "id": "199e1ff6"
   },
   "source": [
    "#### TDR vs %NF (**ROC**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272c448",
   "metadata": {
    "id": "4272c448"
   },
   "outputs": [],
   "source": [
    "tdr_pNF = plot_roc_NNet(TDR, pNF, xlabel='Percent Non-Fraud', ylabel='Transaction Detection Rate (%)', f1=blindholdoutFile, legend=trainOrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b757147",
   "metadata": {
    "id": "7b757147"
   },
   "source": [
    "#### TVDR vs %NF (**Dollar Weighted ROC**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685927e",
   "metadata": {
    "id": "6685927e"
   },
   "outputs": [],
   "source": [
    "tvdr_pNF = plot_roc_NNet(TVDR, pNF, xlabel='Percent Non-Fraud', ylabel='Transaction Value Detection Rate (%)', f1=blindholdoutFile, legend=trainOrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0597c9",
   "metadata": {
    "id": "fd0597c9"
   },
   "source": [
    "##### ADR vs A%NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4264e",
   "metadata": {
    "id": "a6d4264e"
   },
   "outputs": [],
   "source": [
    "adr_afpr = plot_roc_NNet(ADR, ApNF, xlabel='Account % Non-Fraud', ylabel='Account Detection Rate (%)', f1=blindholdoutFile, legend=trainOrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e112",
   "metadata": {
    "id": "3114e112"
   },
   "source": [
    "##### RTVDR vs A%NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6bb8e",
   "metadata": {
    "id": "e7e6bb8e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rtvdr_apnf = plot_roc_NNet(RTVDR, ApNF, xlabel='Account % Non-Fraud', ylabel='Real-Time Value Detection Rate (%)', f1=blindholdoutFile, legend=trainOrTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd31a6",
   "metadata": {
    "id": "f2fd31a6"
   },
   "source": [
    "### Score Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99155580",
   "metadata": {
    "id": "99155580",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_scoreDist_NNet(df_test[df_test['is_train']==0], f1=blindholdoutFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1303218",
   "metadata": {},
   "source": [
    "# Aside from plots, make statements like the following\n",
    "\n",
    "- Our model captured X% Fraud Transactions and prevented Y% Fraud Loss at a 0.5% NF review rate\n",
    "- Our model captured X% Fraud Accounts and prevented Y% Fraud Loss at a 1% NF Account review rate\n",
    "\n",
    "Recall, **Review Rate** is the Total # of Accounts with score >= Score Threshold divided by the Total # of Accounts. From a buisness persepctive, its not feasible to review all False Positives (FPs), so there is a trade off. The goal is to get the least amount of FPs but have the highest amount of True Positive (TPs).\n",
    "\n",
    "Be sure to understand the meaning of what you're stating. Its fair game to be questioned, in detail, on any statements you claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c4170",
   "metadata": {},
   "source": [
    "## Understanding the Lists Produced from calcMetrics\n",
    "\n",
    "The variable called **threshold_list** is a list of all the thresholds that calcMetrics will calculate the metrics for. If **threshold_list** is [0, 5, 10, 15, 20], then it has 5 elements, (i.e., len(threshold_list)=5). This means, the lists' outputted from calcMetrics (i.e., pNF, TDR, TVDR, ApNF, ADR, RTVDR), will each have 5 elements, where each elements values will correspond to the respected threshold value in **threshold_list**. \n",
    "\n",
    "Here is an example, using index numbers, to help understand the values produced. Say, **threshold_list** = [0, 5, 10, 15, 20]. Index value of 0 corresponds to the list value 0, index value of 1 corresponds to the list value of 5, index value of 2 corresponds to the list value of 10, etc. In code form:\n",
    "- threshold_list[0] is 0\n",
    "- threshold_list[1] is 5\n",
    "- threshold_list[2] is 10\n",
    "- threshold_list[3] is 15\n",
    "- threshold_list[4] is 20\n",
    "\n",
    "When calcMetrics is called, it sequentially processes each element in **threshold_list**, one at a time. The pNF, TDR, TVDR, ApNF, ADR, RTVDR values, specific to the elements threshold value, are generated and stored to their respective lists.\n",
    "\n",
    "The above needs to be understood so that you can produce the types of statements provided above. To get the TDR and TVDR values at a 0.5% NF review rate, you need to find the index value of 0.5 in the pNF list. Similary, to get the ADR and RTVDR values at a 1% NF review rate, you need to find the index value of 1 in the ApNF list. Below are example codes that can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b420e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(lst, target):\n",
    "    # Use the `min` function with a custom key to find the closest value\n",
    "    closest_index = min(range(len(lst)), key=lambda i: abs(lst[i] - target))\n",
    "    return closest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42635906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values in pNF, but keep in mind we want the index of the pNF value closest to 0.5\n",
    "pNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84194bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then get the index so you can use that index to get the values in TDR and TVDR at that pNF value\n",
    "idx_pNF = find_closest_index(pNF, 0.5)\n",
    "print(f'NF review rate = {pNF[idx_pNF]}% at idx = {idx_pNF}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the index to generate your results\n",
    "print(f'Our model captured {TDR[idx_pNF]}% Fraud Transactions and prevented {TVDR[idx_pNF]}% Fraud Loss at a {pNF[idx_pNF]}% NF review rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values in ApNF, but keep in mind we want the index of the ApNF value closest to 1\n",
    "ApNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44039088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then get the index so you can use that index to get the values in ADR and RTVDR at that ApNF value\n",
    "idx_ApNF = find_closest_index(ApNF, 1)\n",
    "print(f'NF Account review rate = {ApNF[idx_ApNF]}% at idx = {idx_ApNF}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the index to generate your results\n",
    "print(f'Our model captured {ADR[idx_ApNF]}% Fraud Account and prevented {RTVDR[idx_ApNF]}% Fraud Loss at a {ApNF[idx_ApNF]}% NF Account review rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0154b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
