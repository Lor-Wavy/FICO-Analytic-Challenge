{"cells":[{"cell_type":"markdown","id":"n1M7iVRNm7SE","metadata":{"id":"n1M7iVRNm7SE"},"source":["# **FICO Analytic Challenge © Fair Isaac 2024**"]},{"cell_type":"markdown","id":"2c6396a5-6257-4a01-8dbd-941c45fedcec","metadata":{"id":"2c6396a5-6257-4a01-8dbd-941c45fedcec"},"source":["# Week 5 - Logistic Regression - Model training and evaluation"]},{"cell_type":"markdown","id":"953f6df1-e17c-4500-8944-1edcfe8444cd","metadata":{"id":"953f6df1-e17c-4500-8944-1edcfe8444cd"},"source":["## Logistic Regression\n","\n","Logistic regression is a Machine Learning algorithm used primarily for binary classification problems, where the outcome can take one of two possible values. The goal is to predict the probability that the outcome belongs to a given class or not based on information from predictor variables.\n","\n","Few examples of binary classification problems -\n","1. Spam Detection: To predict if an email is spam or not spam\n","2. Medical Diagnosis: To predict if a tumor is malignant or not\n","3. Marketing: To predict if a customer will buy a product or not\n","4. **Credit Scoring**: To predict if a customer will default on a loan or not\n","5. **Fraud Detection**: To identify if a transaction is fraud or not\n","\n","\n","\n","<img src = https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-05-11-51-17.png width = \"800\" style=\"margin:50px 0px 50px 0px\">\n","\n","<img src = https://www.saedsayad.com/images/LogReg_1.png width = \"1000\" style=\"margin:50px 0px 50px 0px\">\n","\n","**Sigmoid function:** The S-shaped curve used to predict probabilities. It's value is always between 0 and 1. <br>\n","<img src = https://editor.analyticsvidhya.com/uploads/642295.png style=\"margin:0px 50px 20px 250px\">\n","\n","<img src = https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-05-10-58-02.png width = \"800\" style=\"margin:50px 0px 50px 0px\">\n","\n","**Resources:**\n","\n","https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html\n","\n","https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/\n","\n","https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/"]},{"cell_type":"markdown","id":"91d7ccf0-bad4-4242-a678-8ee0554df0f8","metadata":{"id":"91d7ccf0-bad4-4242-a678-8ee0554df0f8"},"source":["## Contents\n","\n","**1. Load Dataset**\n","\n","**2. Modelling Data Preparation**\n","\n","    2.1 Summary Statistics\n","    2.2 Missing value analysis\n","    2.3 Normalization of the features\n","    2.4 Dataset filtering\n","    2.5 Create train and test datasets\n","    \n","**3. Model Training**\n","\n","    3.1 Training Logistic Regression Model\n","    3.2 Forward Selection of features\n","    3.3 Backward Elimination of features"]},{"cell_type":"markdown","id":"22146ae4-53b4-4a5b-8973-5e118b67c9f5","metadata":{"id":"22146ae4-53b4-4a5b-8973-5e118b67c9f5"},"source":["## 1. Load Dataset\n","\n","1. Load the train and test datasets\n","2. Create a new column 'is_train' to use it as a tag to identify train and test datasets\n","3. Combine the train and test datasets for further analysis"]},{"cell_type":"code","execution_count":null,"id":"55d0eee7-1e57-4aa3-8f6a-1e4e49a8567c","metadata":{"collapsed":true,"id":"55d0eee7-1e57-4aa3-8f6a-1e4e49a8567c"},"outputs":[],"source":["! pip install mlxtend"]},{"cell_type":"code","execution_count":null,"id":"fab171d5-8294-43b2-bedf-09192dc3c3d0","metadata":{"collapsed":true,"id":"fab171d5-8294-43b2-bedf-09192dc3c3d0"},"outputs":[],"source":["! pip install seaborn"]},{"cell_type":"code","execution_count":null,"id":"ce56a5c6-34e7-4346-91c8-1b467c2806a6","metadata":{"id":"ce56a5c6-34e7-4346-91c8-1b467c2806a6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pickle import dump, load\n","\n","pd.set_option('display.max_columns', None)   # displays all columns\n","pd.set_option('display.max_rows', None)    # displays all rows"]},{"cell_type":"code","execution_count":null,"id":"6fc83cbc-9112-421d-a996-ab011cfd4459","metadata":{"id":"6fc83cbc-9112-421d-a996-ab011cfd4459"},"outputs":[],"source":["# Setting up the Google Drive mount\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","\n","path = '/content/drive/MyDrive/FICO Analytic Challenge/'\n","os.chdir(path)\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"id":"f0492b8b-478a-4472-b664-e64ee47f426e","metadata":{"id":"f0492b8b-478a-4472-b664-e64ee47f426e"},"outputs":[],"source":["# Location of the data\n","data = 'Data'\n","\n","# Location to save model\n","model_folder = 'Model'\n","\n","# # Names of the datasets\n","# train_dataset_name = \"train_features.csv\"\n","# test_dataset_name = \"test_A_features.csv\"\n","\n","# Name of the model\n","model = 'LogReg'\n","\n","# dataset file prefix\n","trainFile = ['train']\n","testFile = ['test_A']\n","# testFile = ['test_B']\n","# testFile = ['test_C_notags']\n","\n","# CSV filename and where features dataset will be saved\n","featureTrainFileSuffix=\"_advanced_features.csv\"\n","featureTestFileSuffix=\"_advanced_features.csv\"\n","\n","filePathTrain=os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n","filePathTest=os.path.join(path + data, testFile[0] + featureTestFileSuffix)\n","\n","if not os.path.isfile(filePathTrain):\n","    featureTrainFileSuffix=\"_features.csv\"\n","    filePathTrain=os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n","\n","if not os.path.isfile(filePathTest):\n","    featureTestFileSuffix=\"_features.csv\"\n","    filePathTest=os.path.join(path + data, testFile[0] + featureTestFileSuffix)\n","\n","# CSV filename and where outputs will be saved\n","trainsaveCSV = os.path.join(path + data, 'score.' + model + '.' + trainFile[0] + featureTrainFileSuffix)\n","testsaveCSV = os.path.join(path + data, 'score.' + model + '.' + testFile[0] + featureTestFileSuffix)\n","\n","print(\"Path to Train Output file and filename: {}\".format(trainsaveCSV))\n","print(\"Path to Test Output file and filename: {}\".format(testsaveCSV))"]},{"cell_type":"code","execution_count":null,"id":"a2e74bde","metadata":{"id":"a2e74bde"},"outputs":[],"source":["def import_df(filename):\n","    df1 = pd.read_csv(filename)\n","    df1['transactionDateTime'] = pd.to_datetime(df1['transactionDateTime'])\n","    df1 = df1.sort_values(by=['pan','transactionDateTime'])\n","    return df1"]},{"cell_type":"code","execution_count":null,"id":"55ad6dcb-f0db-4708-8236-f292e58e0653","metadata":{"id":"55ad6dcb-f0db-4708-8236-f292e58e0653"},"outputs":[],"source":["# Read the train dataset, print the dimensions (#rows x #columns) of the dataset and view the first 5 rows\n","df_train_features = import_df(filePathTrain)\n","# df_train_features = pd.read_csv(os.path.join(path, data, train_dataset_name))\n","print(df_train_features.shape)\n","df_train_features.head()"]},{"cell_type":"code","execution_count":null,"id":"6a32c26f-173b-497a-b6d5-220b2c6659d4","metadata":{"id":"6a32c26f-173b-497a-b6d5-220b2c6659d4"},"outputs":[],"source":["# Read the test dataset, print the dimensions (#rows x #columns) of the dataset and view the first 5 rows\n","df_test_features = import_df(filePathTest)\n","# df_test_features = pd.read_csv(os.path.join(path, data, test_dataset_name))\n","print(df_test_features.shape)\n","df_test_features.head()"]},{"cell_type":"code","execution_count":null,"id":"0627a025-ddef-4800-b07f-42b0c4da08c6","metadata":{"id":"0627a025-ddef-4800-b07f-42b0c4da08c6"},"outputs":[],"source":["# Combine train and test datasets for feature analysis and further processing\n","df = pd.concat([df_train_features, df_test_features], ignore_index=True, axis=0)\n","df.shape"]},{"cell_type":"code","execution_count":null,"id":"79bec985-e441-4279-aab7-34d17e243e0e","metadata":{"id":"79bec985-e441-4279-aab7-34d17e243e0e"},"outputs":[],"source":["# # Drop the columns which are not needed for modelling\n","# def modify_df(df1):\n","#     # UPDATE THIS PART, ONLY WITH COLUMNS THAT ARE NOT NEEDED\n","#     df1.drop(columns=['transactionDateTime',\n","#                       'trans_num',\n","#                       'unix_time',\n","#                       'merchCountry',\n","#                       'merchState',\n","#                       'merch_lat',\n","#                       'merch_long',\n","#                       'city_pop',\n","#                       'street',\n","#                       'gender',\n","#                       'deltaTime',\n","#                       'job',\n","#                       'dob',\n","#                       'zip',\n","#                       'lat',\n","#                       'long'\n","#                      ],\n","#              inplace= True)\n","\n","#     df1['datetime'] = pd.to_datetime(df1['datetime']).astype('datetime64[ns]')\n","#     df1.rename(columns = {'datetime':'transactionDateTime'}, inplace = True)\n","\n","#     return df1\n","\n","# df = modify_df(df)\n","# df.shape"]},{"cell_type":"markdown","id":"419c65a3-0a63-4eec-b9db-94c5cf99e6fa","metadata":{"id":"419c65a3-0a63-4eec-b9db-94c5cf99e6fa"},"source":["#### Analyze transaction level fraud rates"]},{"cell_type":"code","execution_count":null,"id":"e4ea541e-07d9-453c-8b53-4869d1a736cb","metadata":{"id":"e4ea541e-07d9-453c-8b53-4869d1a736cb"},"outputs":[],"source":["# Analyze distribution of mdlIsFraudTrx\n","print(df['mdlIsFraudTrx'].value_counts(dropna = False))\n","\n","# Analyze distribution of mdlIsFraudTrx as percentage - Fraud rate\n","print(df['mdlIsFraudTrx'].value_counts(dropna = False, normalize = True))"]},{"cell_type":"markdown","id":"6b461a1a-42ae-48a4-b7dc-afed63735e2e","metadata":{"id":"6b461a1a-42ae-48a4-b7dc-afed63735e2e"},"source":["#### Analyze account level fraud rates"]},{"cell_type":"code","execution_count":null,"id":"745a3c5c-f152-458f-8d97-9b7fda78469a","metadata":{"id":"745a3c5c-f152-458f-8d97-9b7fda78469a"},"outputs":[],"source":["# Number of unique pan ids\n","print(df['pan'].nunique())\n","\n","# Create account level dataset by retaining unique pan ids\n","df_account = df[['pan','mdlIsFraudAcct']].sort_values(by = 'mdlIsFraudAcct',ascending = False).drop_duplicates('pan', keep = 'first')\n","print(df_account.shape)"]},{"cell_type":"code","execution_count":null,"id":"6fce141e-2d6c-4f4c-a47b-541c9feb216c","metadata":{"id":"6fce141e-2d6c-4f4c-a47b-541c9feb216c"},"outputs":[],"source":["# Analyze distribution of mdlIsFraudAcct\n","print(df_account['mdlIsFraudAcct'].value_counts(dropna = False))\n","\n","# Analyze distribution of mdlIsFraudAcct as percentage - Fraud rate\n","print(df_account['mdlIsFraudAcct'].value_counts(dropna = False, normalize = True))"]},{"cell_type":"markdown","id":"43b97deb-1531-4593-9990-cd9d9ecfd032","metadata":{"id":"43b97deb-1531-4593-9990-cd9d9ecfd032"},"source":["#### Create Card Present/Card Not Present flag"]},{"cell_type":"code","execution_count":null,"id":"a67b1981-8447-4c57-9dd1-61101de79547","metadata":{"id":"a67b1981-8447-4c57-9dd1-61101de79547"},"outputs":[],"source":["# Create flag is_CNP which takes values 1 for card not present and 0 for card present\n","df['is_CNP'] = (df['category'].apply(lambda x: x[-3:] == 'net')).astype(int)\n","df['is_CNP'].value_counts(dropna = False)"]},{"cell_type":"code","execution_count":null,"id":"d0507d7f-fa65-4e3a-8656-afda48391238","metadata":{"id":"d0507d7f-fa65-4e3a-8656-afda48391238"},"outputs":[],"source":["# Fraud rates for CP vs CNP\n","print(pd.crosstab(df['is_CNP'],df['mdlIsFraudTrx']))\n","print(pd.crosstab(df['is_CNP'],df['mdlIsFraudTrx'], normalize = 'index'))"]},{"cell_type":"code","execution_count":null,"id":"53a63962-8659-4434-b45d-3bb2336a5145","metadata":{"id":"53a63962-8659-4434-b45d-3bb2336a5145"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"a213b216-b253-4040-91f9-c037e8fc83e3","metadata":{"id":"a213b216-b253-4040-91f9-c037e8fc83e3"},"source":["#### Create Features dataset and Target data\n","\n","**Features** - Features are also known as predictors, independent variables, or input variables. These are the attributes of the data that are used to make predictions. They are the inputs to the model. Features are derived from information available at the time of prediction. Features are generally represented by X.\n","\n","**Target** - Target is also known as the response, dependent variable, or output variable. It is the value or label that the model is trying to predict. It is the output of the model. Target is generally represented by y."]},{"cell_type":"code","execution_count":null,"id":"720cab25-c3fd-44f7-9c7c-8d3713a98e3a","metadata":{"id":"720cab25-c3fd-44f7-9c7c-8d3713a98e3a"},"outputs":[],"source":["# Columns that are in the dataframe that aren't inputs to the model, aside from the the columns dropped when importing the dataset\n","base_cols = ['pan', 'merchant', 'category', 'transactionAmount', 'first', 'last',\n","       'mdlIsFraudTrx', 'mdlIsFraudAcct', 'is_train', 'cardholderCountry',\n","       'cardholderState', 'transactionDateTime']\n","\n","feature_columns = list(set(df.columns) - set(base_cols+['is_CNP']))\n","feature_columns.sort()\n","\n","print('Number of features : ',len(feature_columns))\n","print('features : ',feature_columns)\n","\n","# Assign all predictor variables (features) to X and target variable to y\n","X = df[feature_columns].copy()\n","print(X.shape)\n","y = df['mdlIsFraudTrx']"]},{"cell_type":"code","execution_count":null,"id":"3e20eaae-7305-4f07-bbef-f0e6a7ce6cbb","metadata":{"id":"3e20eaae-7305-4f07-bbef-f0e6a7ce6cbb"},"outputs":[],"source":["# Analyze distribution of target variable\n","y.value_counts(dropna = False)"]},{"cell_type":"code","execution_count":null,"id":"5f6e7034-419b-4533-96c1-b2fddb052a17","metadata":{"id":"5f6e7034-419b-4533-96c1-b2fddb052a17"},"outputs":[],"source":["# Analyze distribution of target variable as percentage - Fraud rate\n","y.value_counts(dropna = False, normalize = True)"]},{"cell_type":"code","execution_count":null,"id":"VbmJtV7ZiJjR","metadata":{"id":"VbmJtV7ZiJjR"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0392979d-3423-402b-a65e-819dd6a6a4e7","metadata":{"id":"0392979d-3423-402b-a65e-819dd6a6a4e7"},"source":["## 2. Modelling data preparation"]},{"cell_type":"markdown","id":"b693e3d4-b443-48cf-98fa-697b5265b0ac","metadata":{"id":"b693e3d4-b443-48cf-98fa-697b5265b0ac"},"source":["### 2.1 Summary Statistics\n","\n","Analyze the univariate statistics like min, max, median, percentile distribution etc for all the predictive features"]},{"cell_type":"code","execution_count":null,"id":"4bc4b56a-8a8f-4fb1-b33f-7d46f5116fed","metadata":{"id":"4bc4b56a-8a8f-4fb1-b33f-7d46f5116fed"},"outputs":[],"source":["print(\"Summary Statistics\")\n","summary_statistics = X.describe().T\n","summary_statistics"]},{"cell_type":"code","execution_count":null,"id":"f7cd2f1f-40d4-4173-afcb-838fa106a814","metadata":{"id":"f7cd2f1f-40d4-4173-afcb-838fa106a814"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e512db5e-0d1a-40f8-8cc3-b725fe5ea42c","metadata":{"id":"e512db5e-0d1a-40f8-8cc3-b725fe5ea42c"},"source":["### 2.2 Missing value analysis\n","\n","There are various methods to handle missing data. For example,\n","- If the proportion of missing values in a column is beyond a tolerable limit, those columns can be excluded from the model\n","- If the missing values of a column are within tolerable limit, they are imputed with median or mean value of the column\n","\n","**isna()** is used to identify missing values in the dataset. It returns boolean values - 'True' indicates missing value, 'Flase' indicates non-missing values<br>\n","**fillna()** is used to replace missing/null values with a specified value"]},{"cell_type":"code","execution_count":null,"id":"9e5f54e5-5467-44e9-83bb-05157a7606d8","metadata":{"id":"9e5f54e5-5467-44e9-83bb-05157a7606d8"},"outputs":[],"source":["# Calculate the proportion of missing values in each column\n","X.isna().mean()"]},{"cell_type":"code","execution_count":null,"id":"e888bf1f-cb08-4c51-8559-6cabd730d074","metadata":{"id":"e888bf1f-cb08-4c51-8559-6cabd730d074"},"outputs":[],"source":["# Remove columns with high missing values beyond a threshold\n","# Threshold value can be changed\n","threshold_missing = 0.2\n","\n","X = X.loc[:, X.isna().mean() < threshold_missing]\n","X.shape"]},{"cell_type":"code","execution_count":null,"id":"6cc60a1b-808a-4513-b045-695a80d222a3","metadata":{"id":"6cc60a1b-808a-4513-b045-695a80d222a3"},"outputs":[],"source":["## Impute missings with median value for variables below missing threshold\n","print(X.median())\n","X = X.fillna(X.median())"]},{"cell_type":"code","execution_count":null,"id":"fe79017e-2e80-400f-a6aa-d47f6eacec8a","metadata":{"id":"fe79017e-2e80-400f-a6aa-d47f6eacec8a"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b101c571-6b24-4981-bc51-e69c2eaf2192","metadata":{"id":"b101c571-6b24-4981-bc51-e69c2eaf2192"},"source":["### 2.3 Normalization of the features\n","\n","Normalization is done to transform the freatures to a same scale. This helps in stable model training and easy interpretability of feature importance.\n","\n","There are different ways to normalize data. More details on why normalization is required and different ways to normalize the data can be found here -\n","https://www.datacamp.com/tutorial/normalization-in-machine-learning\n","\n","**StandardScaler** function from sklean is used to normalize the features. It uses below formula to normalize the features.\n","\n","x_transform = (x-mean)/standard deviation"]},{"cell_type":"code","execution_count":null,"id":"f4acc426-d13f-414b-8cf3-3acb803efe63","metadata":{"id":"f4acc426-d13f-414b-8cf3-3acb803efe63"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_transform = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n","print(X_transform.shape)\n","X_transform.head()"]},{"cell_type":"code","execution_count":null,"id":"94da37b6-7839-4f75-92d8-26d8de14e2cb","metadata":{"id":"94da37b6-7839-4f75-92d8-26d8de14e2cb"},"outputs":[],"source":["# Save the scaling parameters\n","scaleFile = os.path.join(path, data , 'scaler.' + model + '.' + data + \".pkl\")\n","dump(scaler, open(scaleFile, 'wb'))"]},{"cell_type":"code","execution_count":null,"id":"76c1cbf4-0f0d-4fe7-afaa-b9c2fbde581f","metadata":{"id":"76c1cbf4-0f0d-4fe7-afaa-b9c2fbde581f"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"5606e986-71f5-44e7-bb17-7cad911dae10","metadata":{"id":"5606e986-71f5-44e7-bb17-7cad911dae10"},"source":["### 2.4 Dataset filtering\n","\n","The following transactions needs to be excluded from modelling data.\n"," - Transactions which are in the first two months to allow for profile maturation\n"," - All the non-fraud transactions corresponding to a fraud account are excluded from the model training"]},{"cell_type":"markdown","id":"810b6817-fd39-40ac-8fb8-5999d8973a2e","metadata":{"id":"810b6817-fd39-40ac-8fb8-5999d8973a2e"},"source":["#### Removing first two months transactions to allow for profile maturation\n","Since some of our profile variables depend on potentially long periods of time, we would like to allow those features to fully build up to their entire calculation window. Ideally, we would have several months before the training period to allow for these features to mature, but since we only have a year's worth of data, we will restrict ourselves to a maximum of 2 month window, allowing our data to train on the remaining 10 months of data. These initial 2 months are known as the ‘profile maturation period’, where these profile variables properly develop. We exclude the transactions from the first 2 months from the modelling data to allow for profile maturation.\n","\n","<font color='red'>**Do not modify the function**</font>"]},{"cell_type":"code","execution_count":null,"id":"f8631962-10f5-4e86-8f4e-e4cf5d7f0816","metadata":{"id":"f8631962-10f5-4e86-8f4e-e4cf5d7f0816"},"outputs":[],"source":["# Function to create boolean variable which tags transactions in the first two months as 'False' and transactions after two months as 'True'\n","def matureProf_n_months(df1, datetime_col, n_months=2):\n","    # Find earliest date in dataset\n","    min_date = df1[datetime_col].min()\n","    # Calculate cutoff date by adding n_months to min_date\n","    cutoff_date = min_date + pd.DateOffset(months=n_months)\n","    print('Earliest date: ', min_date)\n","    print('Cutoff date: ', cutoff_date)\n","\n","    # return a boolean column which takes 'True' for rows where the datetime is less than the cutoff time, otherwise 'False'\n","    return df1[datetime_col] >= cutoff_date\n","\n","# Create boolean variable which tags transactions in the first two months as 'False' and transactions after two months as 'True'\n","profileMature_bool = matureProf_n_months(df, 'transactionDateTime', n_months=2)"]},{"cell_type":"code","execution_count":null,"id":"62a5f506-ebae-4219-a982-11389169ca4c","metadata":{"id":"62a5f506-ebae-4219-a982-11389169ca4c"},"outputs":[],"source":["# Remove the transactions from first two months using profileMature_bool\n","X_profileMature = X_transform[profileMature_bool]\n","y_profileMature = y[profileMature_bool]\n","\n","df_profileMature = df[profileMature_bool]"]},{"cell_type":"code","execution_count":null,"id":"7cf68f94-b9a1-4057-aa1e-5651b001399b","metadata":{"id":"7cf68f94-b9a1-4057-aa1e-5651b001399b"},"outputs":[],"source":["print(X_profileMature.shape)\n","print(y_profileMature.shape)\n","print(df_profileMature.shape)"]},{"cell_type":"markdown","id":"22353476-abb1-43e6-b36d-08d5abfb46b4","metadata":{"id":"22353476-abb1-43e6-b36d-08d5abfb46b4"},"source":["#### Removing Non-fraud transactions from Fraud accounts\n","A fraud account can have fraud transactions and non-fraud transactions. To avoid any uncertainty of these non-fraud transactions being fraud or not, we remove all non-fraud transactions of a fraud account from modelling data."]},{"cell_type":"code","execution_count":null,"id":"250979d6-c867-425e-b9b4-598bcb647244","metadata":{"id":"250979d6-c867-425e-b9b4-598bcb647244"},"outputs":[],"source":["# Generate cross-frequency of mdlIsFraudAcct and mdlIsFraudTrx\n","pd.crosstab(df_profileMature['mdlIsFraudAcct'], df_profileMature['mdlIsFraudTrx'])"]},{"cell_type":"code","execution_count":null,"id":"ca042abe-9a2c-4cc2-ad88-42013815bcb5","metadata":{"id":"ca042abe-9a2c-4cc2-ad88-42013815bcb5"},"outputs":[],"source":["# Create a boolean variable which tags non-fraud transactions of a fraud account as 'False' and the rest of the transactions as 'True'\n","filter_bool = ~((df_profileMature['mdlIsFraudAcct']==1) & (df_profileMature['mdlIsFraudTrx']==0))\n","filter_bool.value_counts(dropna = False)"]},{"cell_type":"code","execution_count":null,"id":"d68dec61-17fe-4560-b016-1b5fc23f72af","metadata":{"id":"d68dec61-17fe-4560-b016-1b5fc23f72af"},"outputs":[],"source":["# Use the filter_bool variable to filter the features and target datasets\n","X_filtered = X_profileMature[filter_bool]\n","y_filtered = y_profileMature[filter_bool]\n","\n","print(X_filtered.shape)\n","print(y_filtered.shape)"]},{"cell_type":"code","execution_count":null,"id":"87900645-a26f-4f4a-b735-32939f125b0e","metadata":{"id":"87900645-a26f-4f4a-b735-32939f125b0e"},"outputs":[],"source":["# Filtering the main dataset\n","df_filtered = df_profileMature.loc[filter_bool, :]\n","df_filtered.shape"]},{"cell_type":"code","execution_count":null,"id":"9894e817-56c1-4f97-bc76-59a1fdcc57c0","metadata":{"id":"9894e817-56c1-4f97-bc76-59a1fdcc57c0"},"outputs":[],"source":["# Unique pan ids\n","df_filtered['pan'].nunique()"]},{"cell_type":"code","execution_count":null,"id":"92499fb4-0da6-4529-8404-2eeead5672f1","metadata":{"id":"92499fb4-0da6-4529-8404-2eeead5672f1"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2bc2c0c1-873c-43fd-9b26-c2a41fd7cecf","metadata":{"id":"2bc2c0c1-873c-43fd-9b26-c2a41fd7cecf"},"source":["### 2.5 Create train and test datasets\n","\n","Train dataset is used to **train the model** and test dataset is used to **evaluate the model** performance. The train and test datasets are chosen randomly so that both datasets represents the  distributions in overall data. Having a test data independent of the train data to evaluate the model also reduces risk of over-fitting of the model.\n","\n","https://www.geeksforgeeks.org/training-data-vs-testing-data/\n","\n","For the purpose of fraud modelling, we need to make sure that all the transactions corresponding to an account (pan id) are part of either train data or test data. Otherwise, there will be profile leaks if transactions from same account are included in both train and test datasets. In that case test data cannot be considered as independent of train data for the sake of evaluation\n","\n","**GroupShuffleSplit** function from sklearn is used to create train and test datasets while ensuring that the accounts from same group (pan) fall into either train or test data"]},{"cell_type":"code","execution_count":null,"id":"f4609498-064c-4f3a-a8a7-d0984ea2424d","metadata":{"id":"f4609498-064c-4f3a-a8a7-d0984ea2424d"},"outputs":[],"source":["# from sklearn.model_selection import GroupShuffleSplit\n","\n","# gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=100)\n","\n","# for train_idx, test_idx in gss.split(X_filtered, y_filtered, groups=df_filtered['pan']):\n","#     # splitting features dataset and target dataset into train and test\n","#     X_train, X_test = X_filtered.iloc[train_idx], X_filtered.iloc[test_idx]\n","#     y_train, y_test = y_filtered.iloc[train_idx], y_filtered.iloc[test_idx]\n","\n","#     # splitting the filtered dataset\n","#     df_train, df_test = df_filtered.iloc[train_idx], df_filtered.iloc[test_idx]"]},{"cell_type":"code","execution_count":null,"id":"48ef8cf6-ddfc-4d94-beec-b63d163d5f27","metadata":{"id":"48ef8cf6-ddfc-4d94-beec-b63d163d5f27"},"outputs":[],"source":["# Use is_train field to split the data into train and test samples\n","\n","# Create boolean variables for train and test\n","train_bool = (df_filtered['is_train']==1)\n","test_bool = (df_filtered['is_train']==0)\n","\n","# Use the boolean variable to create train data\n","X_train = X_filtered.loc[train_bool]\n","y_train = y_filtered.loc[train_bool]\n","df_train = df_filtered.loc[train_bool]\n","\n","# Use the boolean variable to create test data\n","X_test = X_filtered.loc[test_bool]\n","y_test = y_filtered.loc[test_bool]\n","df_test = df_filtered.loc[test_bool]"]},{"cell_type":"code","execution_count":null,"id":"34a2986d-29f1-4a49-9c92-3c1134b7312a","metadata":{"id":"34a2986d-29f1-4a49-9c92-3c1134b7312a"},"outputs":[],"source":["print('X_train :', X_train.shape)\n","print('X_test :', X_test.shape)\n","print('y_train :', y_train.shape)\n","print('y_test :', y_test.shape)"]},{"cell_type":"markdown","id":"8a717c1c-e0cd-4cb6-bb49-1e00a7b28012","metadata":{"id":"8a717c1c-e0cd-4cb6-bb49-1e00a7b28012"},"source":["#### Analyze transaction level fraud rates in train and test"]},{"cell_type":"code","execution_count":null,"id":"2e750a4c-33a5-4c4a-b896-1ccb8a57140d","metadata":{"id":"2e750a4c-33a5-4c4a-b896-1ccb8a57140d"},"outputs":[],"source":["print('Target rate (transaction fraud rate) in y_train : ', y_train.mean())\n","print('Target rate (transaction fraud rate) in y_test : ', y_test.mean())"]},{"cell_type":"markdown","id":"a3866b42-b431-4131-a87a-a97d4b2a1b27","metadata":{"id":"a3866b42-b431-4131-a87a-a97d4b2a1b27"},"source":["#### Analyze CP vs CNP transaction level fraud rates in train and test"]},{"cell_type":"code","execution_count":null,"id":"8ae8696c-2b2a-4593-82c9-f0375ba184d2","metadata":{"id":"8ae8696c-2b2a-4593-82c9-f0375ba184d2"},"outputs":[],"source":["pd.crosstab(df_train['is_CNP'], df_train['mdlIsFraudTrx'], normalize = 'index')"]},{"cell_type":"code","execution_count":null,"id":"5afe0d3b-7be1-4b09-9cee-e0caa9f77802","metadata":{"id":"5afe0d3b-7be1-4b09-9cee-e0caa9f77802"},"outputs":[],"source":["pd.crosstab(df_test['is_CNP'], df_test['mdlIsFraudTrx'], normalize = 'index')"]},{"cell_type":"markdown","id":"c1161100-8c91-4bea-a39c-d0f84a2af8a2","metadata":{"id":"c1161100-8c91-4bea-a39c-d0f84a2af8a2"},"source":["#### Analyze account level fraud rates"]},{"cell_type":"code","execution_count":null,"id":"b245eb06-c607-4089-af73-0fde5d94c699","metadata":{"id":"b245eb06-c607-4089-af73-0fde5d94c699"},"outputs":[],"source":["##-----Create account level train dataset-----------\n","# Number of unique pan ids in train datset\n","print('unique pan ids in train: ',df_train['pan'].nunique())\n","\n","# Number of unique fraud pan ids in train datset\n","print('unique fraud pan ids in train: ',df_train[df_train['mdlIsFraudAcct']==1]['pan'].nunique())\n","\n","# Create account level dataset for train by retaining unique pan ids\n","df_train_account = df_train[['pan','mdlIsFraudAcct']].sort_values(by = 'mdlIsFraudAcct',ascending = False).drop_duplicates('pan', keep = 'first')\n","print('Account level train dataset shape:', df_train_account.shape)\n","\n","##-----Create account level test dataset-----------\n","# Number of unique pan ids in test datset\n","print('unique pan ids in test: ',df_test['pan'].nunique())\n","\n","# Number of unique fraud pan ids in test datset\n","print('unique fraud pan ids in test: ',df_test[df_test['mdlIsFraudAcct']==1]['pan'].nunique())\n","\n","# Create account level dataset for test by retaining unique pan ids\n","df_test_account = df_test[['pan','mdlIsFraudAcct']].sort_values(by = 'mdlIsFraudAcct',ascending = False).drop_duplicates('pan', keep = 'first')\n","print('Account level test dataset shape:', df_test_account.shape)"]},{"cell_type":"code","execution_count":null,"id":"2f6dc285-57ca-404c-b299-b75290b2b245","metadata":{"id":"2f6dc285-57ca-404c-b299-b75290b2b245"},"outputs":[],"source":["print('Account level fraud rate in train: ', df_train_account['mdlIsFraudAcct'].mean())\n","print('Account level fraud rate in test: ', df_test_account['mdlIsFraudAcct'].mean())"]},{"cell_type":"code","execution_count":null,"id":"a9ef7401-ff23-41db-8d1e-60e0b5ab7347","metadata":{"id":"a9ef7401-ff23-41db-8d1e-60e0b5ab7347"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f943ce4a-582b-4e3d-999e-af91152e72fb","metadata":{"id":"f943ce4a-582b-4e3d-999e-af91152e72fb"},"source":["## 3. Model Training"]},{"cell_type":"markdown","id":"42f015e5-a8df-44c6-9c7e-7159c770bb42","metadata":{"id":"42f015e5-a8df-44c6-9c7e-7159c770bb42"},"source":["### 3.1 Training Logistic Regression Model\n","\n","**LogisticRegression** function from sklearn in used to train a Log Reg model\n","\n","More details on LogisticRegression can be found here - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"]},{"cell_type":"code","execution_count":null,"id":"8b1cb0ad-d472-4932-bb2c-34b98e5fc199","metadata":{"id":"8b1cb0ad-d472-4932-bb2c-34b98e5fc199"},"outputs":[],"source":["# import the function\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"id":"f5f07131-1769-4b38-acc0-5895cf953ace","metadata":{"id":"f5f07131-1769-4b38-acc0-5895cf953ace"},"outputs":[],"source":["# initialize the model\n","LR = LogisticRegression(solver='liblinear', random_state=40, max_iter=500)\n","\n","#Train the LR model using train data\n","LR.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"179280c7-333c-40fb-a76b-2ef127802505","metadata":{"id":"179280c7-333c-40fb-a76b-2ef127802505"},"source":["#### Evaluating Feature Importance\n","\n","Since the data is normalized, the coefficients of the features represent their importance. The **magnitudes of the coefficients** indicate the strength of the association with the target variable. The **sign of coefficient** indicates the direction of relationship between the feature and target. Positive sign indicates that if the feature value increases, the likelihood of positive class in target increases and vice-versa.\n","\n","**coef_** function is used to fetch the coeffients of features in the model"]},{"cell_type":"code","execution_count":null,"id":"970958ef-6411-43ae-8670-89d472506f49","metadata":{"id":"970958ef-6411-43ae-8670-89d472506f49"},"outputs":[],"source":["# Setting option to display numbers in float format\n","pd.options.display.float_format = '{:f}'.format\n","\n","# Fetching coefficients\n","feature_coefficients = LR.coef_[0]\n","\n","# Create a DataFrame to display feature importance\n","feature_names = X_train.columns\n","df_importance = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Coefficients': feature_coefficients\n","}).sort_values(by='Coefficients', ascending=False)\n","\n","df_importance"]},{"cell_type":"markdown","id":"aaa71879-9fdd-419e-9f6e-15fd510c89dc","metadata":{"id":"aaa71879-9fdd-419e-9f6e-15fd510c89dc"},"source":["The features with highest magnitude of coefficient influence the predicted target the most. For features with sign of coefficient as negative, higher values are associated with negative outcome. Similarly, for features having sign of coefficient as positive, higher values are associated with positive outcome."]},{"cell_type":"markdown","id":"49f25ac2-239b-4bce-9b68-d341f129e2a7","metadata":{"id":"49f25ac2-239b-4bce-9b68-d341f129e2a7"},"source":["#### Generate predictions and convert to score\n","When the Logistic Regression model is applied on a transaction to make predictions, the output of the model gives is the probability of the transaction being Fraud or Non-Fraud. For operational purposes, these probabilities are converted to a score ranging from 1 - 999. High score indicates that the transaction has high probability of being a fraud and low score indicates low probability of transaction being a fraud. The following function uses the trained Logistic Regression model to generate probabilities on the transaction data and convert the probabilities to score.\n","\n","<font color='red'>**Do not modify the function**</font>"]},{"cell_type":"code","execution_count":null,"id":"248ebf36-af1f-4652-949c-8dd59922fada","metadata":{"id":"248ebf36-af1f-4652-949c-8dd59922fada"},"outputs":[],"source":["# generate predictions and scores\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def scoring_predictions_logreg(X, LR):\n","    ##------- Generate predictions on the data ----#\n","    predictions = pd.Series(LR.predict_proba(X)[:,1])\n","\n","    ##------ convert predictions to score ----#\n","    scaler = MinMaxScaler(feature_range=(1, 999))\n","\n","    # Converting probabilites to logOdds to get a distribution about origin (0)\n","    log_odds = predictions.apply(lambda p: np.log(0.99999/(1-0.99999)) if p == 1 else np.log(p/(1-p)))\n","    score = pd.Series(scaler.fit_transform(log_odds.values[:, None]).astype(int).flatten())\n","\n","    print(\"Y pred min = {}\".format(predictions.min()))\n","    print(\"Y pred max = {}\".format(predictions.max()))\n","    print(\"LogOdds min = {}\".format(log_odds.min()))\n","    print(\"LogOdds max = {}\".format(log_odds.max()))\n","    print(\"Score min = {}\".format(score.min()))\n","    print(\"Score max = {}\".format(score.max()))\n","\n","    return score"]},{"cell_type":"code","execution_count":null,"id":"622e067c-68db-4e8a-babb-8c77cf885bc0","metadata":{"id":"622e067c-68db-4e8a-babb-8c77cf885bc0"},"outputs":[],"source":["# generate scores on train dataset\n","score_train = scoring_predictions_logreg(X_train, LR)"]},{"cell_type":"code","execution_count":null,"id":"19d86dfa-dfa7-4571-aaaa-dd0cff9acc6a","metadata":{"id":"19d86dfa-dfa7-4571-aaaa-dd0cff9acc6a"},"outputs":[],"source":["# generate scores on train dataset\n","score_test = scoring_predictions_logreg(X_test, LR)"]},{"cell_type":"markdown","id":"b287b144-4022-4130-abd0-1bf5581ec3f8","metadata":{"id":"b287b144-4022-4130-abd0-1bf5581ec3f8"},"source":["#### Evaluating performance of the model on train and test datasets\n","\n","The ROC-AUC metric (Receiver Operating Characteristic Area Under the Curve) is a performance metric used to evaluate the performance of binary classification models. It measures the ability of a model to distinguish between the positive and negative classes. Higher value indicates that the model is better at distinguishing the positive and negative classes.\n","\n","ROC curve helps in visualizing the performance of the model. It helps us in understanding the fraud capture rates at different thresholds of non-frauds.\n","\n","Detailed information on ROC-AUC can be found here - https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n","\n","**predict_proba** function is used to calculate the probability estimates for positive and negative classes <br>\n","**roc_auc_score** function is used to calculate the roc-auc metric. It takes actual values of the target and probability extimates of positive class as inputs to calculate roc-auc metric.\n","\n","<font color='red'>**Do not modify the function**</font>"]},{"cell_type":"code","execution_count":null,"id":"8835cb9c-9373-456f-bd00-de9d69ff6a95","metadata":{"id":"8835cb9c-9373-456f-bd00-de9d69ff6a95"},"outputs":[],"source":["from sklearn.metrics import roc_curve\n","def plotROC(y_train, y_train_score, y_test, y_test_score, model = 'All Features', target_fraud_rate = None):\n","    # roc curve for models\n","    NF1, F1, thresh1 = roc_curve(y_train, y_train_score, pos_label=1)\n","    NF2, F2, thresh2 = roc_curve(y_test, y_test_score, pos_label=1)\n","\n","    # roc curve for tpr = fpr\n","    random_probs = [0 for i in range(len(y_test))]\n","    p_NF, p_F, _ = roc_curve(y_test, random_probs, pos_label=1)\n","\n","    # plot roc curves\n","    plt.plot(NF1, F1, linestyle='--',color='orange', label='train')\n","    plt.plot(NF2, F2, linestyle='--',color='green', label='test')\n","    plt.plot(p_NF, p_F, linestyle='--', color='blue')\n","\n","    if target_fraud_rate != None:\n","        # Find the Fraud Capture Rate at the 0.5% Non-Fraud Capture Rate\n","        target_NF = target_fraud_rate\n","\n","        idx1 = np.argmin(np.abs(NF1 - target_NF))\n","        target_F1 = F1[idx1]\n","\n","        idx2 = np.argmin(np.abs(NF2 - target_NF))\n","        target_F2 = F2[idx2]\n","\n","        print(f\"Fraud capture rate at {target_NF} of Frauds in train data is : {target_F1}\")\n","        print(f\"Fraud capture rate at {target_NF} of Frauds in test data is : {target_F2}\")\n","        # Plot vertical line at target NF\n","        plt.axvline(x=target_NF, ymin=0, ymax=target_F1, color='red', linestyle='--', label=f'FPR = {target_NF * 100:.1f}%')\n","        # Plot horizontal line at corresponding F\n","        plt.axhline(y=target_F1, xmin=0, xmax=target_NF,color='red', linestyle='--')\n","\n","        plt.xlim(0, min(target_fraud_rate*10, 1))\n","\n","    # title\n","    plt.title('ROC curve : '+model)\n","    # x label\n","    plt.xlabel('% Non-Frauds')\n","    # y label\n","    plt.ylabel('% Frauds')\n","\n","    plt.legend(loc='best')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"id":"aae5ddb9-e2bf-4f91-898e-0d5d175acb90","metadata":{"id":"aae5ddb9-e2bf-4f91-898e-0d5d175acb90"},"outputs":[],"source":["plotROC(y_train, score_train, y_test, score_test, model = 'All Features')"]},{"cell_type":"code","execution_count":null,"id":"bb1680f7-f552-4550-87b0-0cfd2030cebe","metadata":{"id":"bb1680f7-f552-4550-87b0-0cfd2030cebe"},"outputs":[],"source":["# target_fraud_rate can be changed\n","plotROC(y_train, score_train, y_test, score_test, model = 'All Features', target_fraud_rate = 0.005)"]},{"cell_type":"code","execution_count":null,"id":"aa7debb1-9cd1-419e-8c97-ab135f61d559","metadata":{"id":"aa7debb1-9cd1-419e-8c97-ab135f61d559"},"outputs":[],"source":["# import roc_auc_score\n","from sklearn.metrics import roc_auc_score"]},{"cell_type":"code","execution_count":null,"id":"81243f35-1163-41db-b728-728c2857d9fa","metadata":{"id":"81243f35-1163-41db-b728-728c2857d9fa"},"outputs":[],"source":["# Train data performance\n","\n","auc_train = roc_auc_score(y_train, score_train)\n","print(\"AUC value of the Model on train data : \", auc_train)\n","\n","lauc_train = roc_auc_score(y_train, score_train, max_fpr=0.02)\n","print(\"LAUC value of the Model on train data : \", lauc_train)"]},{"cell_type":"code","execution_count":null,"id":"5a561826-ab2b-4b6c-85b4-a2578339aa48","metadata":{"id":"5a561826-ab2b-4b6c-85b4-a2578339aa48"},"outputs":[],"source":["# Test data performance\n","\n","auc_test = roc_auc_score(y_test, score_test)\n","print(\"AUC value of the Model on test data: \", auc_test)\n","\n","lauc_test = roc_auc_score(y_test, score_test, max_fpr=0.02)\n","print(\"LAUC value of the Model on test data : \", lauc_test)"]},{"cell_type":"markdown","id":"aaab7c40-2da8-4d59-8e02-034fa063b238","metadata":{"id":"aaab7c40-2da8-4d59-8e02-034fa063b238"},"source":["#### Performance of Card Present and Card Not Present on test data"]},{"cell_type":"code","execution_count":null,"id":"95d20934-ef33-4180-90e4-2fff26d30c2d","metadata":{"id":"95d20934-ef33-4180-90e4-2fff26d30c2d"},"outputs":[],"source":["# Card Present\n","auc_test_CP = roc_auc_score(y_test[df_test['is_CNP']==0], score_test.values[df_test['is_CNP']==0])\n","print(\"AUC value of the Model on test data for Card Present Transactions: \", auc_test_CP)\n","\n","lauc_test_CP = roc_auc_score(y_test[df_test['is_CNP']==0], score_test.values[df_test['is_CNP']==0], max_fpr=0.02)\n","print(\"LAUC value of the Model on test data for Card Present Transactions: \", lauc_test_CP)\n","\n","# Card not Present\n","auc_test_CNP = roc_auc_score(y_test[df_test['is_CNP']==1], score_test.values[df_test['is_CNP']==1])\n","print(\"AUC value of the Model on test data for Card Not Present Transactions: \", auc_test_CNP)\n","\n","lauc_test_CNP = roc_auc_score(y_test[df_test['is_CNP']==1], score_test.values[df_test['is_CNP']==1], max_fpr=0.02)\n","print(\"LAUC value of the Model on test data for Card Not Present Transactions: \", lauc_test_CNP)"]},{"cell_type":"markdown","id":"0b4fc6d7-8d69-4e3a-9a53-d2d061c06691","metadata":{"id":"0b4fc6d7-8d69-4e3a-9a53-d2d061c06691"},"source":["#### Save the Logistic Regression Model"]},{"cell_type":"code","execution_count":null,"id":"b0b0a25d-d6e0-41d4-9287-ac6471dc08ef","metadata":{"id":"b0b0a25d-d6e0-41d4-9287-ac6471dc08ef"},"outputs":[],"source":["modelFile = os.path.join(path, model_folder, model + '.' + data + \".pkl\")\n","\n","dump(LR, open(modelFile, 'wb'))"]},{"cell_type":"markdown","id":"d589c9db-dfce-4ff1-b64e-7b39f7a31602","metadata":{"id":"d589c9db-dfce-4ff1-b64e-7b39f7a31602"},"source":["#### Generate score on the whole dataset and output the scored out dataset\n","\n","This dataset will be used as a input to perf_metrics notebook"]},{"cell_type":"code","execution_count":null,"id":"3e69da0c-1011-4cad-9b45-3caeedd88b94","metadata":{"id":"3e69da0c-1011-4cad-9b45-3caeedd88b94"},"outputs":[],"source":["# generate probability predictions on whole data\n","df['y_preds'] = pd.Series(LR.predict_proba(X_transform)[:,1])\n","# use the scoring function defined above to generate scores on whole dataset\n","df['score'] = scoring_predictions_logreg(X_transform, LR)\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"VSUYs8Srjv3L","metadata":{"id":"VSUYs8Srjv3L"},"outputs":[],"source":["saveColumns = [*base_cols, *feature_columns, 'y_preds', 'score']\n","print(f\"Columns to save: {saveColumns}\")"]},{"cell_type":"code","execution_count":null,"id":"cdd600b0-6356-4be0-82f1-737fe2589723","metadata":{"id":"cdd600b0-6356-4be0-82f1-737fe2589723"},"outputs":[],"source":["# saveCSV_train = os.path.join(path + data, 'score.' + model + '.' + train_dataset_name)\n","# saveCSV_test = os.path.join(path + data, 'score.' + model + '.' + test_dataset_name)\n","\n","df[df['is_train']==1][saveColumns].to_csv(trainsaveCSV, index=False)\n","df[df['is_train']==0][saveColumns].to_csv(testsaveCSV, index=False)"]},{"cell_type":"code","execution_count":null,"id":"5ed7305c-090d-4120-9cbd-dd9deeb71960","metadata":{"id":"5ed7305c-090d-4120-9cbd-dd9deeb71960"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"00a18bb8-cabf-4af2-9462-01f1d67668ef","metadata":{"id":"00a18bb8-cabf-4af2-9462-01f1d67668ef"},"source":["### 3.2 Forward Selection of features\n","\n","Forward Selection is a type of feature selection technique that starts with an empty model and adds features one by one based on a specific criterion, typically the model's performance metric. The features are added until a stop criteria is met, like maximum number of features to add or no further imporvemnet in performance.\n","\n","**SequentialFeatureSelector** function from mlxtend is used to add features in Forward selection or remove features in Backward elimination (discussed in next section).\n","\n","More details on SequentialFeatureSelector can be found here - https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#overview"]},{"cell_type":"code","execution_count":null,"id":"1be3cf84-52e6-4be6-a080-d7ad4b6c92fb","metadata":{"id":"1be3cf84-52e6-4be6-a080-d7ad4b6c92fb"},"outputs":[],"source":["from mlxtend.feature_selection import SequentialFeatureSelector\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"id":"21503af6-bce5-4e39-8f51-c01b2857feb2","metadata":{"id":"21503af6-bce5-4e39-8f51-c01b2857feb2"},"outputs":[],"source":["## initialize logistic regression model\n","LR_forward = LogisticRegression(solver='liblinear', random_state=10)"]},{"cell_type":"markdown","id":"Sh1Zzd7vkfFE","metadata":{"id":"Sh1Zzd7vkfFE"},"source":["<font color='red'>**This step may take upto 1 hour to run**</font>"]},{"cell_type":"code","execution_count":null,"id":"4cd632da-06ba-4a59-9bd8-e9654e6f5ff0","metadata":{"id":"4cd632da-06ba-4a59-9bd8-e9654e6f5ff0","scrolled":true},"outputs":[],"source":["# Number of features to select can be changed\n","num_features_to_select_forward = 10\n","\n","sfs_forward = SequentialFeatureSelector(LR_forward, k_features=num_features_to_select_forward, scoring='roc_auc',forward=True, floating=False, cv=5)\n","sfs_forward.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"319d48d8-54fe-4930-a769-7bba2c160dbf","metadata":{"id":"319d48d8-54fe-4930-a769-7bba2c160dbf"},"source":["#### Analyzing selected features\n","<font color='red'>**(Do not modify)**</font>"]},{"cell_type":"markdown","id":"523d5176-02d7-4737-ab74-63d973f64c88","metadata":{"id":"523d5176-02d7-4737-ab74-63d973f64c88"},"source":["**k_feature_names_** gives list of final variables selected by SequentialFeatureSelector"]},{"cell_type":"code","execution_count":null,"id":"5c5bec43-f89a-4e65-9f1a-2e92cada8207","metadata":{"id":"5c5bec43-f89a-4e65-9f1a-2e92cada8207"},"outputs":[],"source":["selected_features_forward = list(sfs_forward.k_feature_names_)\n","print(f\"Selected features: {selected_features_forward}\")"]},{"cell_type":"markdown","id":"655e1a4c-e687-45ec-b7e1-e2b7fbcd9d75","metadata":{"id":"655e1a4c-e687-45ec-b7e1-e2b7fbcd9d75"},"source":["**get_metric_dict** provides summary of iterations"]},{"cell_type":"code","execution_count":null,"id":"677f8a9f-aa53-4ae1-91f5-ab99ff6c1b33","metadata":{"id":"677f8a9f-aa53-4ae1-91f5-ab99ff6c1b33"},"outputs":[],"source":["# Create DataFrame to store the results\n","results_forward = pd.DataFrame.from_dict(sfs_forward.get_metric_dict()).T[['feature_idx','feature_names','avg_score']]\n","results_forward.rename(columns = {'avg_score':'roc'}, inplace = True)\n","results_forward"]},{"cell_type":"markdown","id":"902aa201-634d-4206-87e9-9f463d1249a1","metadata":{"id":"902aa201-634d-4206-87e9-9f463d1249a1"},"source":["Each row in the above dataset represents one model. The columns feature_idx and feature_names shows index numbers and names of all the features included in the model, the column roc shows the roc_auc of the model. At each step, a feature is added to the model which gives best performance."]},{"cell_type":"code","execution_count":null,"id":"48e0597c-066b-456c-8bc0-f00447a64954","metadata":{"id":"48e0597c-066b-456c-8bc0-f00447a64954","scrolled":true},"outputs":[],"source":["# Print the selected features and the corresponding model performance\n","selected_features = results_forward['feature_names'].apply(lambda x: list(x))\n","model_performance = results_forward['roc']\n","\n","list_added_features = []\n","for i, (features, score) in enumerate(zip(selected_features, model_performance)):\n","    print(f\"Step {i+1}:\")\n","    if i>0:\n","        added_feature = [x for x in features if x not in selected_features[i]][0]\n","        print(f\"Added feature(s): {added_feature}\")\n","    else:\n","        added_feature = features[0]\n","        print(f\"Added feature(s): {added_feature}\")\n","    print(f\"Model performance (roc): {score}\")\n","    print(\"-\" * 30)\n","    list_added_features = list_added_features+[added_feature]"]},{"cell_type":"code","execution_count":null,"id":"7a53030b-0537-412f-9920-d4160419e614","metadata":{"id":"7a53030b-0537-412f-9920-d4160419e614"},"outputs":[],"source":["plt.plot(list_added_features, model_performance,label='roc_auc')\n","plt.ylim(0.5, 1)\n","plt.xticks(rotation = 90)\n","# x label\n","plt.xlabel('Feature added at each step')\n","# y label\n","plt.ylabel('roc_auc')\n","plt.show()"]},{"cell_type":"markdown","id":"e1a2a9cc-eca0-4279-b4b7-3a6acaa820ce","metadata":{"id":"e1a2a9cc-eca0-4279-b4b7-3a6acaa820ce"},"source":["As variables are added to the model at each step, the performance of the model increases initially. The rate of performance improvement decreases with each iteration. After a certain point, no further significant improvement is observed."]},{"cell_type":"markdown","id":"e23095ba-6ca8-4b65-b491-58d6ef4a610b","metadata":{"id":"e23095ba-6ca8-4b65-b491-58d6ef4a610b"},"source":["#### Train a Log Reg model with selected features and evaluate the performance"]},{"cell_type":"code","execution_count":null,"id":"ea5f8236-e7a0-4dd1-b469-e213b341bd7d","metadata":{"id":"ea5f8236-e7a0-4dd1-b469-e213b341bd7d"},"outputs":[],"source":["# Creating train and test feature datasets with only the selected features from forward selection method\n","X_train_forward = X_train[selected_features_forward]\n","X_test_forward = X_test[selected_features_forward]\n","print(X_train_forward.shape)\n","print(X_test_forward.shape)"]},{"cell_type":"code","execution_count":null,"id":"2f0015d6-74d6-43aa-a4c0-af1652be0e77","metadata":{"id":"2f0015d6-74d6-43aa-a4c0-af1652be0e77"},"outputs":[],"source":["# Train the model using selected features\n","LR_forward.fit(X_train_forward, y_train)"]},{"cell_type":"code","execution_count":null,"id":"a2656df1-ed82-4f84-ab9f-13bf5558ac60","metadata":{"id":"a2656df1-ed82-4f84-ab9f-13bf5558ac60"},"outputs":[],"source":["# Feature Importance\n","# Fetching coefficients\n","feature_coefficients_forward = LR_forward.coef_[0]\n","\n","# Create a DataFrame to display feature importance\n","feature_names_forward = X_train_forward.columns\n","df_importance_forward = pd.DataFrame({\n","    'Feature_forward': feature_names_forward,\n","    'Coefficients_forward': feature_coefficients_forward\n","}).sort_values(by='Coefficients_forward', ascending=False)\n","\n","display(df_importance_forward)"]},{"cell_type":"code","execution_count":null,"id":"d2ff6d1f-f4af-4d68-9975-fbe3e0eb4650","metadata":{"id":"d2ff6d1f-f4af-4d68-9975-fbe3e0eb4650"},"outputs":[],"source":["# Generate scores and evaluate performance on train dataset\n","score_train_forward = scoring_predictions_logreg(X_train_forward, LR_forward)\n","auc_train_forward = roc_auc_score(y_train, score_train_forward)\n","print(\"AUC value of Forward Inclusion Model on train data: \", auc_train_forward)"]},{"cell_type":"code","execution_count":null,"id":"8c8967fe-f6eb-4c62-842c-275b64c7557d","metadata":{"id":"8c8967fe-f6eb-4c62-842c-275b64c7557d"},"outputs":[],"source":["# Generate scores and evaluate performance on test dataset\n","score_test_forward = scoring_predictions_logreg(X_test_forward, LR_forward)\n","auc_test_forward = roc_auc_score(y_test, score_test_forward)\n","print(\"AUC value of Forward Inclusion Model on test data: \", auc_test_forward)"]},{"cell_type":"code","execution_count":null,"id":"77072ebe-f996-41a9-9f2d-c87c52806210","metadata":{"id":"77072ebe-f996-41a9-9f2d-c87c52806210"},"outputs":[],"source":["plotROC(y_train, score_train_forward, y_test, score_test_forward, model = 'Forward Selection')"]},{"cell_type":"code","execution_count":null,"id":"5031ccb7-7e36-4d34-9c6e-305c68a68f3b","metadata":{"id":"5031ccb7-7e36-4d34-9c6e-305c68a68f3b"},"outputs":[],"source":["# Model performance on CP and CNP\n","# Card Present\n","auc_test_CP_forward = roc_auc_score(y_test[df_test['is_CNP']==0], score_test_forward.values[df_test['is_CNP']==0])\n","print(\"AUC value of the Forward Inclusion Model on test data for Card Present Transactions: \", auc_test_CP_forward)\n","\n","# Card not Present\n","auc_test_CNP_forward = roc_auc_score(y_test[df_test['is_CNP']==1], score_test_forward.values[df_test['is_CNP']==1])\n","print(\"AUC value of the Forward Inclusion Model on test data for Card Not Present Transactions: \", auc_test_CNP_forward)"]},{"cell_type":"code","execution_count":null,"id":"3307bc09-83e1-4e15-a60f-c701227f7ad1","metadata":{"id":"3307bc09-83e1-4e15-a60f-c701227f7ad1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1beab122-c9b1-4687-b096-9186d1b4dc61","metadata":{"id":"1beab122-c9b1-4687-b096-9186d1b4dc61"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9cc58eea-e9a9-43c6-a712-51bcc0341bc5","metadata":{"id":"9cc58eea-e9a9-43c6-a712-51bcc0341bc5"},"source":["### 3.3 Backward Elimination of features\n","\n","Backward Elimination is a feature selection method that starts with a model with all the variables and removes the least significant features one by one until a stopping criteria is met."]},{"cell_type":"code","execution_count":null,"id":"48c921d7-6222-4337-be75-08fb1387b2da","metadata":{"id":"48c921d7-6222-4337-be75-08fb1387b2da"},"outputs":[],"source":["## initialize logistic regression model\n","LR_backward = LogisticRegression(solver='liblinear', random_state=10)"]},{"cell_type":"markdown","id":"pcSw-z0xkumk","metadata":{"id":"pcSw-z0xkumk"},"source":["<font color='red'>**This step may take upto 1 hour to run**</font>"]},{"cell_type":"code","execution_count":null,"id":"e909bbd7-5604-4a62-85bc-e509ba98c39d","metadata":{"id":"e909bbd7-5604-4a62-85bc-e509ba98c39d","scrolled":true},"outputs":[],"source":["# Number of features to select can be changed\n","num_features_to_select_backward = 10\n","\n","sfs_backward = SequentialFeatureSelector(LR_backward, k_features=num_features_to_select_backward, scoring='roc_auc',forward=False, floating=False, cv=5)\n","sfs_backward.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"3299400f-c355-42e0-9a9a-52b270f3eaef","metadata":{"id":"3299400f-c355-42e0-9a9a-52b270f3eaef"},"source":["#### Analyzing selected features\n","<font color='red'>**(Do not modify)**</font>"]},{"cell_type":"code","execution_count":null,"id":"3d77aeb8-6532-4a23-89c1-bbe5e76852a7","metadata":{"id":"3d77aeb8-6532-4a23-89c1-bbe5e76852a7"},"outputs":[],"source":["selected_features_backward = list(sfs_backward.k_feature_names_)\n","print(f\"Selected features: {selected_features_backward}\")"]},{"cell_type":"code","execution_count":null,"id":"04f9e904-66e9-44f9-878e-9e07681fcbb6","metadata":{"id":"04f9e904-66e9-44f9-878e-9e07681fcbb6"},"outputs":[],"source":["# Create DataFrame to store the results\n","results_backward = pd.DataFrame.from_dict(sfs_backward.get_metric_dict()).T[['feature_idx','feature_names','avg_score']]\n","results_backward.rename(columns = {'avg_score':'roc'}, inplace = True)\n","results_backward"]},{"cell_type":"markdown","id":"ea297cea-36c5-4b16-9794-eafa1f1be305","metadata":{"id":"ea297cea-36c5-4b16-9794-eafa1f1be305"},"source":["Each row in the above dataset represents one model. The columns feature_idx and feature_names shows index numbers and names of all the features included in the model, the column roc shows the roc_auc of the model. At each step, a feature is removed from the model which least affects the performance."]},{"cell_type":"code","execution_count":null,"id":"47ec5314-f447-49e4-ae3e-318a1491c19c","metadata":{"id":"47ec5314-f447-49e4-ae3e-318a1491c19c"},"outputs":[],"source":["# Print the selected features and the corresponding model performance\n","selected_features = results_backward['feature_names'].apply(lambda x: list(x))\n","model_performance = results_backward['roc']\n","\n","list_removed_features = []\n","for i, (features, score) in enumerate(zip(selected_features, model_performance)):\n","    print(f\"Step {i+1}:\")\n","    if i>0:\n","        removed_feature = [x for x in selected_features[X_train.shape[1]+1-i] if x not in features][0]\n","        print(f\"Removed feature(s): {removed_feature}\")\n","    else:\n","        removed_feature = 'NA'\n","        print(f\"Removed feature(s): {removed_feature}\")\n","    print(f\"Model performance (roc): {score}\")\n","    print(\"-\" * 30)\n","    list_removed_features = list_removed_features+[removed_feature]"]},{"cell_type":"code","execution_count":null,"id":"027a01ca-3e15-4bb2-92c1-b5ea4ad3d547","metadata":{"id":"027a01ca-3e15-4bb2-92c1-b5ea4ad3d547"},"outputs":[],"source":["plt.plot(list_removed_features, model_performance,label='roc_auc')\n","plt.ylim(0.5, 1)\n","plt.xticks(rotation = 90)\n","# x label\n","plt.xlabel('Feature removed at each step')\n","# y label\n","plt.ylabel('roc_auc')\n","plt.show()"]},{"cell_type":"markdown","id":"4274ea3f-a4b0-4dcc-acd0-956ec0f33dbe","metadata":{"id":"4274ea3f-a4b0-4dcc-acd0-956ec0f33dbe"},"source":["As the features are removed at each step, the performance remains almost same initially. As more features are removed, roc starts to decrease slightly. If more features are removed, we can observe that the performance decresases at each step."]},{"cell_type":"markdown","id":"8d8083ba-44e1-40d5-9485-9588284fb8bd","metadata":{"id":"8d8083ba-44e1-40d5-9485-9588284fb8bd"},"source":["#### Train a Log Reg model with selected features and evaluate on test dataset"]},{"cell_type":"code","execution_count":null,"id":"3f1c0f40-f94c-42b8-b625-33c5669a9826","metadata":{"id":"3f1c0f40-f94c-42b8-b625-33c5669a9826"},"outputs":[],"source":["# Creating train and test feature datasets with only the selected features from backward elimination method\n","X_train_backward = X_train[selected_features_backward]\n","X_test_backward = X_test[selected_features_backward]\n","print(X_train_backward.shape)\n","print(X_test_backward.shape)"]},{"cell_type":"code","execution_count":null,"id":"ff6a912f-ebac-4fa3-b794-6e1187ab471a","metadata":{"id":"ff6a912f-ebac-4fa3-b794-6e1187ab471a"},"outputs":[],"source":["# train the model using selected features\n","LR_backward.fit(X_train_backward, y_train)"]},{"cell_type":"code","execution_count":null,"id":"434b8f9a-983f-479a-8c7d-088dfdbb7d5f","metadata":{"id":"434b8f9a-983f-479a-8c7d-088dfdbb7d5f"},"outputs":[],"source":["# Feature Importance\n","# Fetching coefficients\n","feature_coefficients_backward = LR_backward.coef_[0]\n","\n","# Create a DataFrame to display feature importance\n","feature_names_backward = X_train_backward.columns\n","df_importance_backward = pd.DataFrame({\n","    'Feature_backward': feature_names_backward,\n","    'Coefficients_backward': feature_coefficients_backward\n","}).sort_values(by='Coefficients_backward', ascending=False)\n","\n","display(df_importance_backward)"]},{"cell_type":"code","execution_count":null,"id":"80e880c2-d1a0-44dc-91e1-1c6741126ecf","metadata":{"id":"80e880c2-d1a0-44dc-91e1-1c6741126ecf"},"outputs":[],"source":["# Make predictions and evaluate performance on test dataset\n","score_train_backward = scoring_predictions_logreg(X_train_backward, LR_backward)\n","auc_train_backward = roc_auc_score(y_train, score_train_backward)\n","print(\"AUC value of Backward Elimination Model on train data: \", auc_train_backward)"]},{"cell_type":"code","execution_count":null,"id":"61778736-8e3b-4cfb-94f9-dc710ddd4e12","metadata":{"id":"61778736-8e3b-4cfb-94f9-dc710ddd4e12"},"outputs":[],"source":["# Make predictions and evaluate performance on test dataset\n","score_test_backward = scoring_predictions_logreg(X_test_backward, LR_backward)\n","auc_test_backward = roc_auc_score(y_test, score_test_backward)\n","print(\"AUC value of Backward Elimination Model on test data: \", auc_test_backward)"]},{"cell_type":"code","execution_count":null,"id":"f81706ab-cfbf-4663-88a3-d929ca2ee3a4","metadata":{"id":"f81706ab-cfbf-4663-88a3-d929ca2ee3a4"},"outputs":[],"source":["plotROC(y_train, score_train_backward, y_test, score_test_backward, model = 'Backward Elimination')"]},{"cell_type":"code","execution_count":null,"id":"ce7e1621-7760-4c88-bbde-abc5fb148eea","metadata":{"id":"ce7e1621-7760-4c88-bbde-abc5fb148eea"},"outputs":[],"source":["# Model performance on CP and CNP\n","# Card Present\n","auc_test_CP_backward = roc_auc_score(y_test[df_test['is_CNP']==0], score_test_backward.values[df_test['is_CNP']==0])\n","print(\"AUC value of the Backward Elimination Model on test data for Card Present Transactions: \", auc_test_CP_backward)\n","\n","# Card not Present\n","auc_test_CNP_backward = roc_auc_score(y_test[df_test['is_CNP']==1], score_test_backward.values[df_test['is_CNP']==1])\n","print(\"AUC value of the Backward Elimination Model on test data for Card Not Present Transactions: \", auc_test_CNP_backward)"]},{"cell_type":"code","execution_count":null,"id":"303164a1-f6db-4416-88a6-526f292ad5b5","metadata":{"id":"303164a1-f6db-4416-88a6-526f292ad5b5"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f405b31f-01b3-40a2-a80b-266e7ef70d60","metadata":{"id":"f405b31f-01b3-40a2-a80b-266e7ef70d60"},"source":["### Exercise"]},{"cell_type":"markdown","id":"c16e3b5a-ae9d-4f18-8c94-7cd6e4c31fce","metadata":{"id":"c16e3b5a-ae9d-4f18-8c94-7cd6e4c31fce"},"source":["- Train log reg model on new features (at least 10 variables)\n","- Identify important features\n","- Calculate AUC values for train, test, CP and CNP\n","- Prepare Midpoint Report"]},{"cell_type":"code","execution_count":null,"id":"ff138c2a-536e-42ee-880e-a107cf284a41","metadata":{"id":"ff138c2a-536e-42ee-880e-a107cf284a41"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}