{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afv-8E-tBJUJ"
   },
   "source": [
    "#FICO Analytic Challenge Week 8\n",
    "##Â© Fair Isaac 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObrEF4ZQWXWr"
   },
   "source": [
    "#Advanced Feature Generation\n",
    "Feature generation in neural networks for fraud detection involves creating sophisticated features to better identify fraudulent activities. By transforming raw data into meaningful inputs, we improve the model's ability to detect subtle patterns of fraud, enhancing its predictive accuracy and robustness.\n",
    "\n",
    "Purpose of Feature Generation:\n",
    "- Enhance detection by highlighting behaviors associated with fraud.\n",
    "- Improve accuracy by providing the best possible data representations.\n",
    "\n",
    "Types of Features for Fraud Detection:\n",
    "- Temporal Features: Capture transaction timing and frequency to spot unusual patterns.\n",
    "- Relational Features: Analyze relationships between transaction entities to uncover discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41PiLmRaUoUB",
    "outputId": "28779b68-ac61-4c7e-e764-0951d75f7664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining projects path and directory locations\n",
    "path = '/content/drive/MyDrive/FICO Analytic Challenge/'\n",
    "sys.path.append(path +'Data')\n",
    "sys.path.append(path +'Week 04')\n",
    "os.chdir(path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder's name that's holding dataset\n",
    "data = 'Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset file prefix\n",
    "trainFile = ['train']\n",
    "testFile = ['test_A']\n",
    "# testFile = ['test_B']\n",
    "# testFile = ['test_C_notags']\n",
    "\n",
    "# CSV filename and where features dataset will be read \n",
    "featureTrainFileSuffix=\"_advanced_features.csv\"\n",
    "featureTestFileSuffix=\"_advanced_features.csv\"\n",
    "\n",
    "filePathTrain=os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n",
    "filePathTest=os.path.join(path + data, testFile[0] + featureTestFileSuffix)\n",
    "\n",
    "if not os.path.isfile(filePathTrain):\n",
    "    featureTrainFileSuffix=\"_features.csv\"\n",
    "    filePathTrain=os.path.join(path + data, trainFile[0] + featureTrainFileSuffix)\n",
    "\n",
    "if not os.path.isfile(filePathTest):\n",
    "    featureTestFileSuffix=\"_features.csv\"\n",
    "    filePathTest=os.path.join(path + data, testFile[0] + featureTestFileSuffix)\n",
    "\n",
    "# CSV filename and where features dataset will be saved\n",
    "featureTrainFileSaveSuffix=\"_advanced_features.csv\"\n",
    "featureTestFileSaveSuffix=\"_advanced_features.csv\"\n",
    "\n",
    "trainsaveCSV = os.path.join(path + data, trainFile[0] + featureTrainFileSaveSuffix)\n",
    "testsaveCSV = os.path.join(path + data, testFile[0] + featureTestFileSaveSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_df(filename):\n",
    "    df1 = pd.read_csv(filename)\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    df1['datetime'] =  pd.to_datetime(df1['transactionDateTime'])\n",
    "    df1['transactionDateTime'] = pd.to_datetime(df1['transactionDateTime'])\n",
    "    df1 = df1.sort_values(by=['pan','transactionDateTime'])\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPwGeB4vU5YD"
   },
   "outputs": [],
   "source": [
    "# Upload pre-processed data\n",
    "#df1 = pd.read_csv('/content/drive/MyDrive/FICO Analytic Challenge/Data/train_advanced_features.csv')\n",
    "#df = pd.read_csv('/content/drive/MyDrive/FICO Analytic Challenge/Data/test_C_notags_advanced_features.csv')\n",
    "#df = pd.read_csv('/content/drive/MyDrive/FICO Analytic Challenge/Data/test_C_notags_features.csv')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/FICO Analytic Challenge/Data/train_advanced_features.csv')\n",
    "#df = pd.concat([df1, df2])\n",
    "# df = import_df(filePathTrain)\n",
    "# df = import_df(filePathTest)\n",
    "\n",
    "\n",
    "df1 = import_df(filePathTrain)\n",
    "df2 = import_df(filePathTest)\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "base_cols = ['pan', 'merchant', 'category', 'transactionAmount', 'first', 'last',\n",
    "       'mdlIsFraudTrx', 'mdlIsFraudAcct', 'is_train', 'cardholderCountry',\n",
    "       'cardholderState', 'transactionDateTime', 'gender', 'street', 'zip',\n",
    "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', \n",
    "       'merch_lat', 'merch_long', 'merchCountry', 'merchState', 'deltaTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnk5FfMeWJS0"
   },
   "outputs": [],
   "source": [
    "# # ONLY FOR TEST_C_NOTAGS\n",
    "# df = import_df(filePathTest)\n",
    "\n",
    "# # ONLY FOR TEST_C_NOTAGS\n",
    "# base_cols = ['pan', 'merchant', 'category', 'transactionAmount', 'first', 'last', 'is_train', 'cardholderCountry',\n",
    "#        'cardholderState', 'transactionDateTime', 'gender', 'street', 'zip',\n",
    "#        'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', \n",
    "#        'merch_lat', 'merch_long', 'merchCountry', 'merchState', 'deltaTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEoTFeO8ZQwN",
    "outputId": "e8d50f5b-f638-458f-88cc-d2bdab7d9771"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'pan', 'merchant', 'category', 'transactionAmount', 'lat',\n",
       "       'long', 'trans_num', 'unix_time', 'merch_lat', 'merch_long',\n",
       "       'mdlIsFraudTrx', 'mdlIsFraudAcct', 'transactionDateTime', 'is_train',\n",
       "       'merchCountry', 'merchState', 'cardholderCountry', 'cardholderState',\n",
       "       'transactionHour', 'time_since_last_transaction', 'IsHighValue',\n",
       "       'is_international', 'is_cnp', 'is_shoppingnet', 'is_grocery_net',\n",
       "       'is_travel', 'IS_0_TO_5AM', 'average_spending', 'wednesday_buy',\n",
       "       'monday_buy', 'spending_below_avg_20', 'spending_above_threshold',\n",
       "       'outside_state_purchase', 'is_late_night', 'user_avg_amount',\n",
       "       'RelativeAmount', 'deltaTime', 'amt_trend_5e', 'amt_trend_24h',\n",
       "       'count_trend_1h', 'category_ratio', 'repeat_amt', 'index',\n",
       "       'rolling_mean_14D', 'rolling_mean_60D', 'ratio_14D_to_60D', 'ewm_1D',\n",
       "       '1m', 'repeat_hi_amt_1H', 'HighValue_International',\n",
       "       'transactionHour_Risk', 'is_grocery_pos', 'outside_country',\n",
       "       'high_interaction', 'AvgTransactionAmount_Last7Days',\n",
       "       'weekend_weekday_ratio', 'rolling_mean_30D', 'ratio_30D_to_60D',\n",
       "       'datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMM8eO3KUKF3"
   },
   "source": [
    "# Advanced Feature Generation for Credit Card Fraud Detection\n",
    "\n",
    "Though we have already covered somewhat advanced features in week 4, there is still information to be squeezed from the data. We will investigate 3 key concepts in this notebook:\n",
    "* ratio variables\n",
    "* exponential weighting\n",
    "* repeat events\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCh-_QSYbBQ-"
   },
   "source": [
    "## Ratio Variables\n",
    "\n",
    "Ratio variables are perfect for detecting anomalies in spending patterns over time. The problem with simple \"last hour\" or \"last day\" variables is that you have no knowledge about this customer's habits outside of that specified timeframe. It's possible that the user naturally fluctuates rapidly throughout the day or week, but we're lacking the necessary long-period information. Typically you want the numerator to be something which encapsulates a time over which periodic behaviors can undergo a full cycle. For example: a user's spending habits might correlate with their pay cycle (2 weeks), or paying off their bills such as a car payment or rent (1 month). We don't want these non-fraud behaviors to trigger any alarm in our variables, so we make the numerator long enough for it to average out. On the other hand, we want our denominator to capture several of these cycles to ensure the numerator truly is reflective of that user's spending patterns. Please create at least three ratio variables which capture transaction frequency or spending amount averages. You may want to use rolling windows for these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lkVQupGbEl2"
   },
   "outputs": [],
   "source": [
    "# Example: rolling mean of last 14 days (2 weeks) vs. rolling mean of last 60 days (2 months)\n",
    "df.set_index('datetime', inplace=True)  # Set the date column as the index\n",
    "df['rolling_mean_14D'] = df.groupby('pan')['transactionAmount'].rolling(window='14D', min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['rolling_mean_60D'] = df.groupby('pan')['transactionAmount'].rolling(window='60D', min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['ratio_14D_to_60D'] = df['rolling_mean_14D'] / df['rolling_mean_60D']\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx0CnIEpXftL"
   },
   "source": [
    "If you want to alter the range of the window, simply change the window parameter in the rolling function. Here, we've specified '14D' and '60D' respectively. If you want to make a variable which goes over a window of a month, for instance, you could replace the parameter with '30D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Zki-RyhcH_W"
   },
   "outputs": [],
   "source": [
    "# Making sure that NaN values are replaced with zeros because they can cause issues with our rolling average calculations\n",
    "df = df.fillna(0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0navlsMuHPrT"
   },
   "outputs": [],
   "source": [
    "# # Visualizing ratio variables that we have calulated\n",
    "# pan_number = df['pan'].unique()[0]  # Select the first pan number for example\n",
    "# subset = df[df['pan'] == pan_number]\n",
    "\n",
    "# # Choose a time range (e.g., first 3 months)\n",
    "# start_date = subset['datetime'].min()\n",
    "# end_date = start_date + pd.DateOffset(months=3)\n",
    "# subset = subset[(subset['datetime'] >= start_date) & (subset['datetime'] <= end_date)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(subset['datetime'], subset['rolling_mean_14D'], label='Rolling Mean (7 days)')\n",
    "# plt.plot(subset['datetime'], subset['transactionAmount'], label='Transaction Amount')\n",
    "# plt.plot(subset['datetime'], subset['rolling_mean_60D'], label='Rolling Mean (30 days)')\n",
    "\n",
    "# plt.title(f'Transaction Amount and Rolling Mean for PAN: {pan_number}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Amount')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH7JtsowWXWt"
   },
   "source": [
    "Now, let's visualize the ratio of the 14-day rolling average and 60 day rolling average. Calculating this ratio can highlight periods where the short-term trend is significantly different from the long-term trend, indicating potential anomalies, seasonal effects, or sudden changes in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_P2wEvcdSDu"
   },
   "outputs": [],
   "source": [
    "# # Plotting ratio of 14-day rolling average to 60-day rolling average\n",
    "# pan_number = df['pan'].unique()[0]  # Select the first pan number for example\n",
    "# subset = df[df['pan'] == pan_number]\n",
    "\n",
    "# # Choose a time range (e.g., first 3 months)\n",
    "# start_date = subset['datetime'].min()\n",
    "# end_date = start_date + pd.DateOffset(months=3)\n",
    "# subset = subset[(subset['datetime'] >= start_date) & (subset['datetime'] <= end_date)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(subset['datetime'], subset['ratio_14D_to_60D'], label='Rolling Ratio')\n",
    "\n",
    "# plt.title(f'Transaction Amount and Rolling Mean for PAN: {pan_number}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Ratio')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY9W0zqcbE2o"
   },
   "source": [
    "## Exponential Weighting\n",
    "\n",
    "We have some notion of how to leverage long-term spending patterns to detect fraud, but what if we wanted to detect the instantaneous frauds as they're occurring? Exponential event and time decay are designed specifically for this task. By giving priority to the most recent transaction, we are able to detect fraud even sometimes on the very first fraud attempt, saving banks even more money.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_U7VBmfWXWt"
   },
   "source": [
    "### Below is an example of the exponential weighted mean function over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79hDrk7JWXWt"
   },
   "outputs": [],
   "source": [
    "df['datetime'] =  pd.to_datetime(df['transactionDateTime'])\n",
    "# df.reset_index(drop=True,inplace=True)\n",
    "# Define halflife in Days\n",
    "halflife = 1.  #Day\n",
    "\n",
    "df['ewm_1D'] = df.groupby('pan').apply(\n",
    "    lambda group: group['transactionAmount'].ewm(halflife=halflife, adjust=False).mean()\n",
    ").reset_index(level=0, drop=True)\n",
    "# Making sure that NaN values are replaced with zeros because they can cause\n",
    "# issues with our rolling average calculations\n",
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SIb_4J8P0Vs"
   },
   "outputs": [],
   "source": [
    "# # Plotting ratio of 14-day rolling average to 60-day rolling average\n",
    "# pan_number = df['pan'].unique()[0]  # Select the first pan number for example\n",
    "# subset = df[df['pan'] == pan_number]\n",
    "\n",
    "# # Choose a time range (e.g., first 3 months)\n",
    "# start_date = subset['datetime'].min()\n",
    "# end_date = start_date + pd.DateOffset(months=3)\n",
    "# subset = subset[(subset['datetime'] >= start_date) & (subset['datetime'] <= end_date)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(subset['datetime'], subset['ewm_1D'], label='Rolling Ratio')\n",
    "# plt.plot(subset['datetime'], subset['transactionAmount'], label='Transaction Amount')\n",
    "\n",
    "# plt.title(f'Transaction Amount and Rolling Mean for PAN: {pan_number}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Ratio')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE4e1piQWXWt"
   },
   "source": [
    "Here, halflife (the time it takes for the value to decay to half) is a parameter which dictates how much you would like to prioritize more recent events. The larger halflife is, the less you prioritize recent events, and the smaller halflife is, the more you emphasize recent events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDSP5s43BaS9"
   },
   "source": [
    "# Different profile filters distinguish different basic profile variable types\n",
    "Examples:\n",
    "- Time Averages\n",
    "- Repeats\n",
    "\n",
    "# Time Averages\n",
    "- Weighted average rate of change over time\n",
    "- Weighting function gives\n",
    "    - Unit weight to current time\n",
    "    - Exponential suppression for earlier events, e.g. e^[(ð‘¡_(ð‘›âˆ’1)âˆ’ð‘¡_ð‘›)/ð‘‡]\n",
    "\n",
    "# Repeats\n",
    "- Counts how many repeat events have a specified property\n",
    "- Normal repeat counting goes as 1, 2, 3, 4, â€¦.\n",
    "- Falcon uses exponentially decayed repeat counting\n",
    "- If current event qualifies as repeats, then multiply prior count by e^(âˆ’dt/T) before adding 1\n",
    "- If current event does not qualify as repeat, then reset variable to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj99JcD-J_dQ"
   },
   "outputs": [],
   "source": [
    "def compute_timedecay_rate(df, decay=3600, name = \"time_decay\"):\n",
    "    avg_amount_list = []\n",
    "    current_count_rate = 0\n",
    "    current_amount_rate = 0\n",
    "    current_pan = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        DC = np.exp(-row[\"deltaTime\"]/decay)\n",
    "        if (row[\"pan\"] != current_pan):\n",
    "            current_pan = row[\"pan\"]\n",
    "            current_count_rate = 0\n",
    "            current_amount_rate = 0\n",
    "            DC = 0\n",
    "        current_count_rate = 1 + current_count_rate*DC\n",
    "        current_amount_rate = row[\"transactionAmount\"] + current_amount_rate*DC\n",
    "        avg_amount = current_amount_rate / current_count_rate\n",
    "        avg_amount_list.append(avg_amount)\n",
    "    df[name] = avg_amount_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wc-iJhu0gArP"
   },
   "outputs": [],
   "source": [
    "def compute_hi_repeat(df, decay=60, name = \"1d\"):\n",
    "\n",
    "    avg_amount_list = []\n",
    "    current_count_rate = 0\n",
    "    current_amount_rate = 0\n",
    "    current_pan = None\n",
    "\n",
    "    df['condition'] = False\n",
    "    df.loc[df['transactionAmount'] > 100., 'condition'] = True\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if ((row[\"pan\"] != current_pan) or (row['condition'] == False)):\n",
    "            current_pan = row[\"pan\"]\n",
    "            current_count_rate = 0\n",
    "            current_amount_rate = 0\n",
    "        current_count_rate = 1 + current_count_rate*np.exp(-row[\"deltaTime\"]/decay)\n",
    "        current_amount_rate = row[\"transactionAmount\"] + current_amount_rate*np.exp(-row[\"deltaTime\"]/decay)\n",
    "        avg_amount = current_amount_rate / current_count_rate\n",
    "        avg_amount_list.append(avg_amount)\n",
    "\n",
    "    df[name] = avg_amount_list\n",
    "    df.drop(columns='condition',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCSfS9c8e9EB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "df['deltaTime'] = df.groupby('pan')['datetime'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds()\n",
    "compute_timedecay_rate(df, decay=60, name = \"1m\")\n",
    "compute_hi_repeat(df, decay=3600, name = \"repeat_hi_amt_1H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "thsTdAdVvxOY",
    "outputId": "ab3df0d1-39a8-4a17-ed66-f62e1e1d2438"
   },
   "outputs": [],
   "source": [
    "df[df['repeat_hi_amt_1H'] < df['transactionAmount']/3]['pan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLQRwDMkZyz2"
   },
   "outputs": [],
   "source": [
    "# #pan_number = df[df['mdlIsFraudAcct'] == 1]['pan'].unique()[29]  # Select the first pan number for example\n",
    "# pan_number = '2712209726293386A'\n",
    "# subset = df[df['pan'] == pan_number]\n",
    "\n",
    "# # Choose a time range (e.g., first 3 months)\n",
    "# start_date = subset['datetime'].min()\n",
    "# end_date = start_date + pd.DateOffset(months=3)\n",
    "# subset = subset[(subset['datetime'] >= start_date) & (subset['datetime'] <= end_date)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(subset['datetime'], subset['1m'], label='1m')\n",
    "# plt.plot(subset['datetime'], subset['transactionAmount'], label='Transaction Amount')\n",
    "\n",
    "# plt.title(f'1D for PAN: {pan_number}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Amount')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(subset['datetime'], subset['repeat_hi_amt_1H'], label='repeat_hi_amt_1H')\n",
    "# plt.plot(subset['datetime'], subset['transactionAmount'], label='Transaction Amount')\n",
    "\n",
    "# plt.title(f'repeat_1D for PAN: {pan_number}')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Amount')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4BKxbqU1KE9"
   },
   "source": [
    "## Now that we have a dataframe with some potentially useful features, let's save it to be used in future sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2Et2elYI2Fc",
    "outputId": "bce34b2f-f249-4dd7-a990-bcb147dd692f"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPnboyMW9lfw"
   },
   "outputs": [],
   "source": [
    "df.set_index('datetime', inplace=True)\n",
    "df['AvgTransactionAmount_Last7Days'] = df.groupby('pan')['transactionAmount'].transform(lambda x: x.rolling(window='168h').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppQ-oRLJgj8B"
   },
   "outputs": [],
   "source": [
    "# df['datetime'] = pd.to_datetime(df['transactionDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCUnobly_uAw"
   },
   "outputs": [],
   "source": [
    "df['HighValue_International'] = ((df['transactionAmount'] > 1000) & (df['is_international'] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-diU9KrZ_t85"
   },
   "outputs": [],
   "source": [
    "df['transactionHour_Risk'] = ((df['transactionHour'].astype(int) < 4) | (df['transactionHour'].astype(int) > 21)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0A27v9Y_t41"
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.set_index('datetime', inplace=True)\n",
    "df['count_trend_1h'] = df.groupby('pan')['transactionAmount'].transform(lambda x: x.rolling(window='1h').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5jGLNcR_t0-"
   },
   "outputs": [],
   "source": [
    "def calculate_category_ratio(df):\n",
    "\n",
    "    # Define a function to compute rolling category ratios for each customer\n",
    "    def compute_ratios(group):\n",
    "        # Create a boolean mask of matches within the past 5 records (rolling window)\n",
    "        match_mask = (group['category'] == group['category'].shift(1))\n",
    "        # Rolling sum of matches for past 5 rows\n",
    "        rolling_sum = match_mask.rolling(window=5, min_periods=1).sum()\n",
    "        # Calculate the ratio\n",
    "        return rolling_sum / rolling_sum.rolling(window=5, min_periods=1).count()\n",
    "\n",
    "    # Apply the function group-wise (for each customer)\n",
    "    df['category_ratio'] = df.groupby('pan', group_keys=False).apply(compute_ratios)\n",
    "\n",
    "    return df\n",
    "df = calculate_category_ratio(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKttDxFn_ttf"
   },
   "outputs": [],
   "source": [
    "# #df['weekend_weekday_ratio'] = df.groupby('pan').apply(lambda x: x[x['datetime'].dt.dayofweek.isin([5, 6])]['transactionAmount'].sum() / x[~x['datetime'].dt.dayofweek.isin([5, 6])]\n",
    "#  ['transactionAmount'].sum()).shift().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvblcSNR_tp3"
   },
   "outputs": [],
   "source": [
    "#df['is_shoppingnet'] = (df['category'].apply(lambda x: x == 'shopping_net')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M003gWTF_tmn"
   },
   "outputs": [],
   "source": [
    "df['is_grocery_pos'] =(df['category'].apply(lambda x: x == 'grocery_pos')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqdqAUDh_tjA"
   },
   "outputs": [],
   "source": [
    "df['is_travel'] = (df['category'] == 'travel') & (df['transactionAmount'] > 400).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lkGT-jh_tfR"
   },
   "outputs": [],
   "source": [
    "df['average_spending'] = df.groupby('pan')['transactionAmount'].transform('mean')\n",
    "# This line is added to calculate and create the 'average_spending' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAFhromIhSTu"
   },
   "outputs": [],
   "source": [
    "# df['weekend_weekday_ratio'] = df.groupby('pan').apply(lambda x: x[x['datetime'].dt.dayofweek.isin([5, 6])]['transactionAmount'].sum() / x[~x['datetime'].dt.dayofweek.isin([5, 6])]\n",
    "#  ['transactionAmount'].sum()).shift().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv6MXhLXa9az"
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df['datetime'] = pd.to_datetime(df['transactionDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61HBIdgE_tQT"
   },
   "outputs": [],
   "source": [
    "df['monday_buy']=((df['datetime'].dt.dayofweek.isin([1]))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuYsusqn_tGb"
   },
   "outputs": [],
   "source": [
    "df['spending_below_avg_20'] = (df['average_spending'] - df['transactionAmount'] >= 20).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xe4usr-_s_a"
   },
   "outputs": [],
   "source": [
    "df['spending_above_threshold'] = (df['transactionAmount'] > df['average_spending'] + 15).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMwb5n6s_sxy"
   },
   "outputs": [],
   "source": [
    "df['outside_state_purchase'] = (df['merchState'] != df['cardholderState']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl0GKBbQ_sqf"
   },
   "outputs": [],
   "source": [
    "df['outside_country']= (df['merchCountry']!= df['cardholderCountry']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmF8aSzV_sh4"
   },
   "outputs": [],
   "source": [
    "df['user_avg_amount'] = df.groupby('pan')['transactionAmount'].expanding().mean().shift().reset_index(level=0, drop=True)\n",
    "\n",
    "#df['user_avg_amount'] = df.groupby('pan')['transactionAmount'].expanding().mean().shift().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wb-JOnQD_sZF"
   },
   "outputs": [],
   "source": [
    "df['high_interaction'] = ((df['transactionAmount'] * df['user_avg_amount']) > df['user_avg_amount'].mean()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl55HicziKsy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6smBMS7HChx"
   },
   "outputs": [],
   "source": [
    "# prompt: Create the 'ratio_30D_to_60D' that uses the last 60 days\n",
    "\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "df['rolling_mean_30D'] = df.groupby('pan')['transactionAmount'].rolling(window='30D', min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['rolling_mean_60D'] = df.groupby('pan')['transactionAmount'].rolling(window='60D', min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['ratio_30D_to_60D'] = df['rolling_mean_30D'] / df['rolling_mean_60D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to save\n",
    "if \"datetime\" in df.columns:\n",
    "  features = list(set(df.columns) - set(base_cols + [\"datetime\"]))\n",
    "else:\n",
    "  features = list(set(df.columns) - set(base_cols))\n",
    "\n",
    "features.sort()\n",
    "saveFeatures = [*base_cols, *features]\n",
    "print(f\"Features to save: {saveFeatures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oESuVelCaAzV"
   },
   "outputs": [],
   "source": [
    "#df[df['is_train'] == 1].to_csv(f'/content/drive/MyDrive/FICO Analytic Challenge/Data/train_advanced_features.csv',index=False)\n",
    "# df[df['is_train'] == 0].to_csv(f'/content/drive/MyDrive/FICO Analytic Challenge/Data/test_C_notags_advanced_features.csv',index=False)\n",
    "\n",
    "df[df['is_train'] == 1][saveFeatures].to_csv(trainsaveCSV ,index=False)\n",
    "df[df['is_train'] == 0][saveFeatures].to_csv(testsaveCSV,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mf_KgGmZbEUd"
   },
   "outputs": [],
   "source": [
    "# # CHALLENGE: rolling mean of last 30 days (1 month) vs. rolling mean of last 60 days (2 months)\n",
    "# df['rolling_mean_30D'] = #TODO\n",
    "# df['ratio_30D_to_60D'] = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mE9N9vEsG2G"
   },
   "outputs": [],
   "source": [
    "# # CHALLENGE: Exponential weighted mean over time with a half-life of 4 days\n",
    "\n",
    "# df['datetime'] =  pd.to_datetime(df['transactionDateTime'])\n",
    "# df.reset_index(drop=True,inplace=True)\n",
    "# # Define halflife in Days\n",
    "# halflife = #TODO\n",
    "\n",
    "# df['ewm_1D'] = df.groupby('pan').apply(\n",
    "#     lambda group: group['transactionAmount'].ewm(halflife=halflife, adjust=False).mean()\n",
    "# ).reset_index(level=0, drop=True)\n",
    "# # Making sure that NaN values are replaced with zeros because they can cause\n",
    "# # issues with our rolling average calculations\n",
    "# df = df.fillna(0)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
